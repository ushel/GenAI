{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6770e05e",
   "metadata": {},
   "source": [
    "Here till now our query has done single forward pass, given the query call the right tool with right parameters and get back the response.\n",
    "\n",
    "what if we ask complex question consisting of multiple steps,or vague question that needs clarification?\n",
    "\n",
    "Instead of tool calling in single shot setting, agent is able to reason over tools and multiple steps. \n",
    "\n",
    "We will use function calling agent implementation, which is agent that natively integrates with the function calling capabilities of LLMs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab1b2d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1854cbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python-dotenv could not parse statement starting at line 1\n",
      "python-dotenv could not parse statement starting at line 8\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f9c2e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d379e003",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "documents = SimpleDirectoryReader(input_files = [\"/Users/apple/Desktop/project/LLM/agentic_RAG/Data/metagpt.pdf\"]).load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8048ff5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "splitter = SentenceSplitter(chunk_size=1024)\n",
    "nodes = splitter.get_nodes_from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "596fa289",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "\n",
    "\n",
    "Settings.llm=OpenAI(model=\"gpt-3.5-turbo\")\n",
    "Settings.embed_model = OpenAIEmbedding(model = \"text-embedding-ada-002\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668e718a",
   "metadata": {},
   "source": [
    "Index tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6eb9ea33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SummaryIndex, VectorStoreIndex\n",
    "\n",
    "summary_index = SummaryIndex(nodes)\n",
    "vector_index = VectorStoreIndex(nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9ba396",
   "metadata": {},
   "source": [
    "Query Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c7f9ec9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_query_engine = summary_index.as_query_engine(\n",
    "    response_mode = \"tree_summarize\",\n",
    "    use_async = True\n",
    ")\n",
    "vector_query_engine = vector_index.as_query_engine()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe2a5b3",
   "metadata": {},
   "source": [
    "convert engine to query tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "671d2dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.tools import QueryEngineTool\n",
    "\n",
    "summary_tool= QueryEngineTool.from_defaults(\n",
    "    name=\"summary_tool\",\n",
    "    query_engine = summary_query_engine,\n",
    "    description = (\n",
    "        \"Useful for summarization questions related to MetaGPT\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "vector_tool= QueryEngineTool.from_defaults(\n",
    "    name=\"vector_tool\",\n",
    "    query_engine = vector_query_engine,\n",
    "    description = (\n",
    "        \"useful for retrieving specific context from the MetaGPT paper.\"\n",
    "    ),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4183c471",
   "metadata": {},
   "source": [
    "Agent\n",
    "\n",
    "1. AgentRunner\n",
    "2. AgentWorker\n",
    "\n",
    "Agent Runner:- overall task dispatcher, which is responsible for creating a task, orchestrating runs of agent worker on top of a given task,\n",
    "\n",
    "and being able to return back the final response to the user.\n",
    "\n",
    "Agent Task Orchestrator\n",
    "\n",
    "AgentState: mapping from task_id to taskstate\n",
    "\n",
    "Taskstate:-\n",
    "\n",
    "1. Task\n",
    "2. Completed steps\n",
    "3. step queue\n",
    "\n",
    "Memory: Conversation memory\n",
    "\n",
    "AgentWorker:- Executing the next step of a given agent\n",
    "\n",
    "Task Reasoning and Execution:- \n",
    "\n",
    "Tools: \n",
    "\n",
    "1. Vector Tool\n",
    "2. Summary Tool\n",
    "\n",
    "LLM: LLM to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c5870c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "llm = OpenAI(model = \"gpt-3.5-turbo\",temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6a5f9019",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.agent import FunctionCallingAgentWorker\n",
    "from llama_index.core.agent import AgentRunner\n",
    "\n",
    "agent_worker = FunctionCallingAgentWorker.from_tools(\n",
    "    [vector_tool, summary_tool],\n",
    "    llm=llm,\n",
    "    verbose=True\n",
    ")\n",
    "agent = AgentRunner(agent_worker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5668518c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Tell me about the agent roles in MetaGPT,and then how they communicate with each other.\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool with args: {\"input\": \"Agent roles in MetaGPT\"}\n",
      "=== Function Output ===\n",
      "The agent roles in MetaGPT are the Product Manager, Architect, Project Manager, Engineer, and QA Engineer. Each role has specific responsibilities and contributes specialized outputs to the collaborative software development process. The Product Manager focuses on requirements analysis and documentation, the Architect designs the technical specifications and system architecture, the Project Manager allocates tasks and oversees project execution, the Engineer implements the code based on the technical specifications, and the QA Engineer formulates test cases to ensure code quality. These roles work together in a structured workflow to efficiently develop software solutions.\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool with args: {\"input\": \"Communication between agent roles in MetaGPT\"}\n",
      "=== Function Output ===\n",
      "Communication between agent roles in MetaGPT is structured and efficient, with each agent having a specific role and expertise. The communication protocol involves structured interfaces and a publish-subscribe mechanism, where agents publish messages in a shared pool and subscribe to relevant messages based on their profiles. This approach ensures transparent exchange of information and allows agents to receive only task-related information. The communication flow is facilitated through natural language, with the Architect designing technical specifications, the Project Manager handling task allocation, the Engineer completing development tasks, and the QA Engineer ensuring software quality through unit test code generation and review. This structured communication approach enhances collaboration efficiency among the agents in MetaGPT.\n",
      "=== LLM Response ===\n",
      "The agent roles in MetaGPT are the Product Manager, Architect, Project Manager, Engineer, and QA Engineer. Each role has specific responsibilities and contributes specialized outputs to the collaborative software development process. The Product Manager focuses on requirements analysis and documentation, the Architect designs the technical specifications and system architecture, the Project Manager allocates tasks and oversees project execution, the Engineer implements the code based on the technical specifications, and the QA Engineer formulates test cases to ensure code quality. These roles work together in a structured workflow to efficiently develop software solutions.\n",
      "\n",
      "Communication between agent roles in MetaGPT is structured and efficient, with each agent having a specific role and expertise. The communication protocol involves structured interfaces and a publish-subscribe mechanism, where agents publish messages in a shared pool and subscribe to relevant messages based on their profiles. This approach ensures transparent exchange of information and allows agents to receive only task-related information. The communication flow is facilitated through natural language, with the Architect designing technical specifications, the Project Manager handling task allocation, the Engineer completing development tasks, and the QA Engineer ensuring software quality through unit test code generation and review. This structured communication approach enhances collaboration efficiency among the agents in MetaGPT.\n"
     ]
    }
   ],
   "source": [
    "response = agent.query(\n",
    "    \"Tell me about the agent roles in MetaGPT,\"\n",
    "    \"and then how they communicate with each other.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee660f40",
   "metadata": {},
   "source": [
    "When you run multi step query like this, you want to make sure that your are actually able to trace the sources. \n",
    "\n",
    "We are able to look at response that source nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "38af0ca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_label: 1\n",
      "file_name: metagpt.pdf\n",
      "file_path: /Users/apple/Desktop/project/LLM/agentic_RAG/Data/metagpt.pdf\n",
      "file_type: application/pdf\n",
      "file_size: 16911937\n",
      "creation_date: 2025-05-31\n",
      "last_modified_date: 2025-05-31\n",
      "\n",
      "Preprint\n",
      "METAGPT: M ETA PROGRAMMING FOR A\n",
      "MULTI -AGENT COLLABORATIVE FRAMEWORK\n",
      "Sirui Hong1∗, Mingchen Zhuge2∗, Jonathan Chen1, Xiawu Zheng3, Yuheng Cheng4,\n",
      "Ceyao Zhang4, Jinlin Wang1, Zili Wang, Steven Ka Shing Yau5, Zijuan Lin4,\n",
      "Liyang Zhou6, Chenyu Ran1, Lingfeng Xiao1,7, Chenglin Wu1†, J¨urgen Schmidhuber2,8\n",
      "1DeepWisdom, 2AI Initiative, King Abdullah University of Science and Technology,\n",
      "3Xiamen University, 4The Chinese University of Hong Kong, Shenzhen,\n",
      "5Nanjing University, 6University of Pennsylvania,\n",
      "7University of California, Berkeley, 8The Swiss AI Lab IDSIA/USI/SUPSI\n",
      "ABSTRACT\n",
      "Remarkable progress has been made on automated problem solving through so-\n",
      "cieties of agents based on large language models (LLMs). Existing LLM-based\n",
      "multi-agent systems can already solve simple dialogue tasks. Solutions to more\n",
      "complex tasks, however, are complicated through logic inconsistencies due to\n",
      "cascading hallucinations caused by naively chaining LLMs. Here we introduce\n",
      "MetaGPT, an innovative meta-programming framework incorporating efficient\n",
      "human workflows into LLM-based multi-agent collaborations. MetaGPT en-\n",
      "codes Standardized Operating Procedures (SOPs) into prompt sequences for more\n",
      "streamlined workflows, thus allowing agents with human-like domain expertise\n",
      "to verify intermediate results and reduce errors. MetaGPT utilizes an assembly\n",
      "line paradigm to assign diverse roles to various agents, efficiently breaking down\n",
      "complex tasks into subtasks involving many agents working together. On col-\n",
      "laborative software engineering benchmarks, MetaGPT generates more coherent\n",
      "solutions than previous chat-based multi-agent systems. Our project can be found\n",
      "at https://github.com/geekan/MetaGPT.\n",
      "1 I NTRODUCTION\n",
      "Autonomous agents utilizing Large Language Models (LLMs) offer promising opportunities to en-\n",
      "hance and replicate human workflows. In real-world applications, however, existing systems (Park\n",
      "et al., 2023; Zhuge et al., 2023; Cai et al., 2023; Wang et al., 2023c; Li et al., 2023; Du et al., 2023;\n",
      "Liang et al., 2023; Hao et al., 2023) tend to oversimplify the complexities. They struggle to achieve\n",
      "effective, coherent, and accurate problem-solving processes, particularly when there is a need for\n",
      "meaningful collaborative interaction (Chen et al., 2024; Zhang et al., 2023; Dong et al., 2023; Zhou\n",
      "et al., 2023; Qian et al., 2023).\n",
      "Through extensive collaborative practice, humans have developed widely accepted Standardized\n",
      "Operating Procedures (SOPs) across various domains (Belbin, 2012; Manifesto, 2001; DeMarco &\n",
      "Lister, 2013). These SOPs play a critical role in supporting task decomposition and effective coor-\n",
      "dination. Furthermore, SOPs outline the responsibilities of each team member, while establishing\n",
      "standards for intermediate outputs. Well-defined SOPs improve the consistent and accurate exe-\n",
      "cution of tasks that align with defined roles and quality standards (Belbin, 2012; Manifesto, 2001;\n",
      "DeMarco & Lister, 2013; Wooldridge & Jennings, 1998). For instance, in a software company,\n",
      "Product Managers analyze competition and user needs to create Product Requirements Documents\n",
      "(PRDs) using a standardized structure, to guide the developmental process.\n",
      "Inspired by such ideas, we design a promising GPT -based Meta-Programming framework called\n",
      "MetaGPT that significantly benefits from SOPs. Unlike other works (Li et al., 2023; Qian et al.,\n",
      "2023), MetaGPT requires agents to generate structured outputs, such as high-quality requirements\n",
      "∗These authors contributed equally to this work.\n",
      "†Chenglin Wu (alexanderwu@fuzhi.ai) is the corresponding author, affiliated with DeepWisdom.\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(response.source_nodes[0].get_content(metadata_mode=\"all\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9dbcad1",
   "metadata": {},
   "source": [
    "Full agent Reasoning loop\n",
    "\n",
    "|<-----------------------------------------------------------------------------------|\n",
    "Query --->   Agent Runner --- > Agent Worker                     |  --> Query Engine | ----> Response\n",
    "       |     Agent Runner < --- Agent Worker    ----> Tools ---->|  --> Query Engine |\n",
    "       |     Reasoning                                                               |\n",
    "       |     & planning layer                                                        |\n",
    "       |-----------------------------------> memory <--------------------------------|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4843456",
   "metadata": {},
   "source": [
    "calling agent query allows you to query the agent in a one off manner, but does not preserve state.\n",
    "\n",
    "lets try maintaining conversation history over time.\n",
    "\n",
    "Agent are able to maintain chats in a conversational memory buffer.  \n",
    "\n",
    "The memory module can be customized, but by default it's a flat list of items that's a rolling buffer depending on the size of the context window of the LLM.\n",
    "\n",
    "Therefore, When the agent decides to use a tool and not only uses a current chat, but also the previous conversation history to take the next step or perform\n",
    "\n",
    "the next action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6bee11da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Tell me about the evaluation datasets used.\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool with args: {\"input\": \"Evaluation datasets used in MetaGPT\"}\n",
      "=== Function Output ===\n",
      "The evaluation datasets used in MetaGPT include HumanEval, MBPP, and a self-generated SoftwareDev dataset. HumanEval consists of 164 handwritten programming tasks, while MBPP comprises 427 Python tasks covering core concepts and standard library features. The SoftwareDev dataset includes 70 representative examples of software development tasks with diverse scopes, such as mini-games, image processing algorithms, and data visualization. These datasets were utilized to evaluate the performance of MetaGPT in code generation tasks.\n",
      "=== LLM Response ===\n",
      "The evaluation datasets used in MetaGPT include HumanEval, MBPP, and a self-generated SoftwareDev dataset. HumanEval consists of 164 handwritten programming tasks, while MBPP comprises 427 Python tasks covering core concepts and standard library features. The SoftwareDev dataset includes 70 representative examples of software development tasks with diverse scopes, such as mini-games, image processing algorithms, and data visualization. These datasets were utilized to evaluate the performance of MetaGPT in code generation tasks.\n"
     ]
    }
   ],
   "source": [
    "response = agent.chat(\n",
    "    \"Tell me about the evaluation datasets used.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f82146ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Tell me the overall results over one of the above datasets\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool with args: {\"input\": \"Overall results of MetaGPT on the HumanEval dataset\"}\n",
      "=== Function Output ===\n",
      "MetaGPT demonstrated superior performance on the HumanEval dataset, surpassing previous methods in terms of functional accuracy, task completion rate, and code quality. Its collaborative framework, which includes role specialization, structured communication, and executable feedback mechanisms, played a significant role in generating high-quality code solutions. Additionally, MetaGPT achieved an average score of 0.724 in the HumanEval benchmark, outperforming GPT-4-0613, which was more responsive to prompts, code parsing, and post-processing outcomes compared to GPT-3.5-Turbo-0613. GPT-4 consistently outperformed GPT-3.5-Turbo-0613 in various settings during the HumanEval experiments.\n",
      "=== LLM Response ===\n",
      "MetaGPT demonstrated superior performance on the HumanEval dataset, surpassing previous methods in terms of functional accuracy, task completion rate, and code quality. Its collaborative framework, which includes role specialization, structured communication, and executable feedback mechanisms, played a significant role in generating high-quality code solutions. Additionally, MetaGPT achieved an average score of 0.724 in the HumanEval benchmark, outperforming GPT-4-0613, which was more responsive to prompts, code parsing, and post-processing outcomes compared to GPT-3.5-Turbo-0613. GPT-4 consistently outperformed GPT-3.5-Turbo-0613 in various settings during the HumanEval experiments.\n"
     ]
    }
   ],
   "source": [
    "response = agent.chat(\n",
    "    \"Tell me the overall results over one of the above datasets\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db6f199",
   "metadata": {},
   "source": [
    "Agent Control\n",
    "\n",
    "\n",
    "Key Benefits:\n",
    "\n",
    "1. Decoupling of Task Creation and Execution:\n",
    "\n",
    "User gain the flexibility to schedule task execution according to their needs.\n",
    "\n",
    "2. Enhanced Debuggability: \n",
    "\n",
    "Offers deeper insights into each step of execution process, improving troubleshooting capabilities.\n",
    "\n",
    "3. Steerability:\n",
    "\n",
    "Allows users to directly modify intermediate steps and incorporate human feedback for refined control.\n",
    "\n",
    "Having this low level agent interface is powerful for two main reasons.\n",
    "\n",
    "1. Debuggability :- Transparency and visibility into what's actually going on under the hood.\n",
    "\n",
    "Especially, say if you agent isn't working the first time around, then you can actually go in and trace the execution of the agent, \n",
    "\n",
    "see where its failing and actually try out different inputs to see if that actually modifies the agent execution into a correct response.\n",
    "\n",
    "2. Richer UXs, where you are building a product experiences around this core agent capability.\n",
    "\n",
    "Example :- You are building a product experience around this core agent capability. Lets say you want to listen to human feedback\n",
    "\n",
    "in the middle of agent execution, as opposed to only after the agent execution is complete for a given task. Then, you can imagine creating some sort of \n",
    "\n",
    "async queue, where you are able to listen to inputs from humans throughout the middle of agent execution.\n",
    "\n",
    "And if human input actually comes in, you can actually interrupt and modify the execution of an agent its going on through a larger task,\n",
    "\n",
    "as opposed to having to wait until the agent task is complete."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a13f72",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "99fcb990",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_worker = FunctionCallingAgentWorker.from_tools(\n",
    "    [vector_tool, summary_tool],\n",
    "    llm = llm,\n",
    "    verbose = True\n",
    ")\n",
    "\n",
    "agent = AgentRunner(agent_worker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2dafa1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = agent.create_task(\n",
    "    \"Tell me about the agent roles in MetaGPT,\"\n",
    "    \"and then how they communicate with each other.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e46dd81f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Tell me about the agent roles in MetaGPT,and then how they communicate with each other.\n",
      "=== Calling Function ===\n",
      "Calling function: vector_tool with args: {\"input\": \"Agent roles in MetaGPT\"}\n",
      "=== Function Output ===\n",
      "In MetaGPT, the agent roles include Product Manager, Architect, Project Manager, Engineer, and QA Engineer. Each role has specific responsibilities and skills tailored to their tasks within the software company simulated by MetaGPT. The roles are designed to collaborate effectively, with each agent contributing specialized outputs to solve complex problems and tasks.\n",
      "=== Calling Function ===\n",
      "Calling function: vector_tool with args: {\"input\": \"How do agents communicate with each other in MetaGPT?\"}\n",
      "=== Function Output ===\n",
      "Agents in MetaGPT communicate with each other by utilizing a shared message pool where they publish structured messages and subscribe to relevant messages based on their profiles. This communication protocol enhances role communication efficiency by allowing agents to exchange messages directly, access messages from other entities transparently, and retrieve required information from the shared pool without the need to inquire about other agents individually. Additionally, agents in MetaGPT use a publish-subscribe mechanism to manage and disseminate information effectively, ensuring that agents receive only task-related information based on their role profiles.\n"
     ]
    }
   ],
   "source": [
    "step_output = agent.run_step(task.task_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "92c77138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num completed for task 9fd7740d-5109-43d5-9276-ad6ccb00681d: 1\n",
      "In MetaGPT, the agent roles include Product Manager, Architect, Project Manager, Engineer, and QA Engineer. Each role has specific responsibilities and skills tailored to their tasks within the software company simulated by MetaGPT. The roles are designed to collaborate effectively, with each agent contributing specialized outputs to solve complex problems and tasks.\n"
     ]
    }
   ],
   "source": [
    "completed_steps = agent.get_completed_steps(task.task_id)\n",
    "print(f\"Num completed for task {task.task_id}: {len(completed_steps)}\")\n",
    "print(completed_steps[0].output.sources[0].raw_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c2640777",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num upcoming steps for task9fd7740d-5109-43d5-9276-ad6ccb00681d: 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TaskStep(task_id='9fd7740d-5109-43d5-9276-ad6ccb00681d', step_id='e7292cb7-2756-4097-b793-9d50906c4855', input=None, step_state={}, next_steps={}, prev_steps={}, is_ready=True)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "upcoming_steps = agent.get_upcoming_steps(task.task_id)\n",
    "print(f\"Num upcoming steps for task{task.task_id}: {len(upcoming_steps)}\")\n",
    "upcoming_steps[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b405f28",
   "metadata": {},
   "source": [
    "input is none because the way agent works is actually just auto generates, action from the conversation history and doesn't need to generate an additional external input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a419808f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: What about how agents share information?\n",
      "=== LLM Response ===\n",
      "Agents in MetaGPT share information by utilizing a shared message pool. This pool allows agents to publish structured messages and access messages from other agents directly. By storing information in this global message pool, agents can exchange information transparently without the need for one-to-one communication. Additionally, agents can subscribe to specific information based on their role profiles, ensuring they receive only relevant information and avoid information overload.\n"
     ]
    }
   ],
   "source": [
    "step_output = agent.run_step(\n",
    "    task.task_id,\n",
    "    input = \"What about how agents share information?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d611c9a",
   "metadata": {},
   "source": [
    "check if the output is the last steps and need to synthesis the answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "493cebb2",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "pop from an empty deque",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m step_output \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtask_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(step_output\u001b[38;5;241m.\u001b[39mis_last)\n",
      "File \u001b[0;32m~/anaconda3/envs/enve/lib/python3.10/site-packages/llama_index/core/instrumentation/dispatcher.py:324\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.wrapper\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    321\u001b[0m             _logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to reset active_span_id: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    323\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 324\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    325\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, asyncio\u001b[38;5;241m.\u001b[39mFuture):\n\u001b[1;32m    326\u001b[0m         \u001b[38;5;66;03m# If the result is a Future, wrap it\u001b[39;00m\n\u001b[1;32m    327\u001b[0m         new_future \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mensure_future(result)\n",
      "File \u001b[0;32m~/anaconda3/envs/enve/lib/python3.10/site-packages/llama_index/core/agent/runner/base.py:490\u001b[0m, in \u001b[0;36mAgentRunner.run_step\u001b[0;34m(self, task_id, input, step, **kwargs)\u001b[0m\n\u001b[1;32m    488\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Run step.\"\"\"\u001b[39;00m\n\u001b[1;32m    489\u001b[0m step \u001b[38;5;241m=\u001b[39m validate_step_from_args(task_id, \u001b[38;5;28minput\u001b[39m, step, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 490\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtask_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatResponseMode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mWAIT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/enve/lib/python3.10/site-packages/llama_index/core/instrumentation/dispatcher.py:324\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.wrapper\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    321\u001b[0m             _logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to reset active_span_id: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    323\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 324\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    325\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, asyncio\u001b[38;5;241m.\u001b[39mFuture):\n\u001b[1;32m    326\u001b[0m         \u001b[38;5;66;03m# If the result is a Future, wrap it\u001b[39;00m\n\u001b[1;32m    327\u001b[0m         new_future \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mensure_future(result)\n",
      "File \u001b[0;32m~/anaconda3/envs/enve/lib/python3.10/site-packages/llama_index/core/agent/runner/base.py:408\u001b[0m, in \u001b[0;36mAgentRunner._run_step\u001b[0;34m(self, task_id, step, input, mode, **kwargs)\u001b[0m\n\u001b[1;32m    406\u001b[0m task \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mget_task(task_id)\n\u001b[1;32m    407\u001b[0m step_queue \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mget_step_queue(task_id)\n\u001b[0;32m--> 408\u001b[0m step \u001b[38;5;241m=\u001b[39m step \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mstep_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpopleft\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28minput\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    410\u001b[0m     step\u001b[38;5;241m.\u001b[39minput \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[0;31mIndexError\u001b[0m: pop from an empty deque"
     ]
    }
   ],
   "source": [
    "step_output = agent.run_step(task.task_id)\n",
    "print(step_output.is_last)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d9c7a943",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = agent.finalize_response(task.task_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1235b44c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agents in MetaGPT share information by utilizing a shared message pool. This pool allows agents to publish structured messages and access messages from other agents directly. By storing information in this global message pool, agents can exchange information transparently without the need for one-to-one communication. Additionally, agents can subscribe to specific information based on their role profiles, ensuring they receive only relevant information and avoid information overload.\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c623189",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "enve",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

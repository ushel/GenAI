{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c33c04c6",
   "metadata": {},
   "source": [
    "In basic RAG pipeline LLMs are only used for synthesis. router shows hoq to use our LLM to make a decision by picking choice of different pipelines.\n",
    "\n",
    "Here we will show you how to use LLM to not only pick a function to execute, but also infer an argument to pass through function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ecc45b",
   "metadata": {},
   "source": [
    "One of the promises of LLms is their ability to take actions and interact with external environment and necessary component to make this possible is a good interface for the LLMs to use.\n",
    "\n",
    "\n",
    "Using Tool calling, in basic RAG pipeline, LLMs are only used for synthesis of information only,\n",
    "\n",
    "we can use LLMs to pick the best query pipeline to answer the user query. This is simplified form of tool calling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25d6f9a",
   "metadata": {},
   "source": [
    "Tool calling enables LLM to interact with the external environments through a dynamic interface where the tool calling not only helps choosing the appropriate tool but also infer necessary arguments for execution.\n",
    "\n",
    "here we will sjow how to use an LLM to not only pick a function to execute, but also infer an argument to pass to function.\n",
    "\n",
    "This allows LLM to how to use vectordb instead of just consuming its outputs. And the final results, is that users are able to ask more questions and get abck more precise results than standard RAG techniques through tool calling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597a8baa",
   "metadata": {},
   "source": [
    "Tool calling adds a layer of query understanding on top of a RAG pipeline, enable user to ask complex queries and get more precise results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0917cb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from dotenv import load_dotenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2d21473",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python-dotenv could not parse statement starting at line 1\n",
      "python-dotenv could not parse statement starting at line 8\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c58a36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aeec5b1",
   "metadata": {},
   "source": [
    "How to find a tool interface from a python function, and the LLM will automatically infer the parameters from the signature of the python functions using LlamaIndex abstractions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c3d504",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculator functions\n",
    "\n",
    "# FunctionTool wraps any given Python Function that you feed it. And so we see that the function tool takes in both the add function defined here,as well as mystery function\n",
    "# which is just (x+y) * (x+y) we can see both add and mystery functions have type annotations for both x and y variables, as well as the docstring.\n",
    "\n",
    "\n",
    "from llama_index.core.tools import FunctionTool\n",
    "\n",
    "def add(x:int, y:int) -> int:\n",
    "    \"\"\"Adds two integers together.\"\"\"  # is used as prompt for the LLM\n",
    "    return x+y\n",
    "\n",
    "def mystery(x:int, y:int) -> int:\n",
    "    \"\"\"Mystery function that operates on top of two numbers.\"\"\"\n",
    "    return (x+y) * (x+y)\n",
    "\n",
    "add_tool = FunctionTool.from_defaults(fn=add)\n",
    "mystery_tool = FunctionTool.from_defaults(fn=mystery)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c728b03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Calling Function ===\n",
      "Calling function: mystery with args: {\"x\": 2, \"y\": 9}\n",
      "=== Function Output ===\n",
      "121\n",
      "121\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
    "response = llm.predict_and_call(\n",
    "    [add_tool, mystery_tool],\n",
    "    \"Tell me the output of the mystery function on 2 and 9\",\n",
    "    verbose = True\n",
    ")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2c5c58",
   "metadata": {},
   "source": [
    "we can say above example is the extended version of the router, not only the LLM pick the tool, but also decides what parameters to give to the tool.\n",
    "\n",
    "Will define Agentic layer on top of vector search, now not only LLM choose vector search we can also get it to infer metadata filters, which is structured list of tags\n",
    "\n",
    "that helps to return a more precise set of search results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "916a5501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the documents\n",
    "\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "documents = SimpleDirectoryReader(input_files = [\"/Users/apple/Desktop/project/LLM/agentic_RAG/Data/metagpt.pdf\"]).load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5dc4fba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the document into set of even chunks or nodes with a chunk size of 1024.\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "splitter = SentenceSplitter(chunk_size=1024)\n",
    "nodes = splitter.get_nodes_from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "17855991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_label: 5\n",
      "file_name: metagpt.pdf\n",
      "file_path: /Users/apple/Desktop/project/LLM/agentic_RAG/Data/metagpt.pdf\n",
      "file_type: application/pdf\n",
      "file_size: 16911937\n",
      "creation_date: 2025-05-31\n",
      "last_modified_date: 2025-05-31\n",
      "\n",
      "Preprint\n",
      "Figure 3: A diagram showing the software development process in MetaGPT, emphasizing its sig-\n",
      "nificant dependence on SOPs. The more detailed demonstration can be found in Appendix B.\n",
      "Specifically, as shown in Figure 1, upon obtaining user requirements, the Product Manager under-\n",
      "takes a thorough analysis, formulating a detailed PRD that includes User Stories and Requirement\n",
      "Pool. This serves as a preliminary functional breakdown. The structured PRD is then passed to\n",
      "the Architect, who translates the requirements into system design components, such as File Lists,\n",
      "Data Structures, and Interface Definitions. Once captured in the system design, the information is\n",
      "directed towards the Project Manager for task distribution. Engineers proceed to execute the des-\n",
      "ignated classes and functions as outlined (detailed in Figure 2). In the following stage, the QA\n",
      "Engineer formulates test cases to enforce stringent code quality. In the final step, MetaGPT pro-\n",
      "duces a meticulously crafted software solution. We provide a detailed schematic (Figure 3) and a\n",
      "concrete instance (Appendix B) of the SOP workflow in MetaGPT.\n",
      "3.2 C OMMUNICATION PROTOCOL\n",
      "Structured Communication Interfaces Most current LLM-based multi-agent frameworks (Li\n",
      "et al., 2023; Zhuge et al., 2023; Zhang et al., 2023; Park et al., 2023) utilize unconstrained natural\n",
      "language as a communication interface.\n",
      "However, despite the versatility of natural language, a question arises: does pure natural language\n",
      "communication suffice for solving complex tasks? For example, in the telephone game (or Chinese\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "print(nodes[5].get_content(metadata_mode=\"all\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dc88fed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "vector_index = VectorStoreIndex(nodes)\n",
    "query_engine = vector_index.as_query_engine(similarity_top_k=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c47b400",
   "metadata": {},
   "source": [
    "Calling RAG pipeline using metadata filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "83fb63fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.vector_stores import MetadataFilters\n",
    "\n",
    "query_engine = vector_index.as_query_engine(\n",
    "    similarity_top_k=2,\n",
    "    filters=MetadataFilters.from_dicts(\n",
    "        [\n",
    "            {\"key\":\"page_label\", \"value\":\"2\"}\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "\n",
    "response = query_engine.query(\n",
    "    \"What are some high-level results of MetaGPT?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "05ac9072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some high-level results of MetaGPT include achieving a new state-of-the-art in code generation benchmarks with 85.9% and 87.7% in Pass@1, standing out in handling higher levels of software complexity, offering extensive functionality, and achieving a 100% task completion rate in experimental evaluations.\n"
     ]
    }
   ],
   "source": [
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0dedd77d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'page_label': '2', 'file_name': 'metagpt.pdf', 'file_path': '/Users/apple/Desktop/project/LLM/agentic_RAG/Data/metagpt.pdf', 'file_type': 'application/pdf', 'file_size': 16911937, 'creation_date': '2025-05-31', 'last_modified_date': '2025-05-31'}\n"
     ]
    }
   ],
   "source": [
    "for n in response.source_nodes:\n",
    "    print(n.metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6c2bfb",
   "metadata": {},
   "source": [
    "# Enhancing Data Retrieval\n",
    "\n",
    "1. Integrating Metadata Filters into a retrieval tool function. :- This function takes in both the query string and page numbers as filters.\n",
    "    \n",
    "    The LLM then actually infer the page numbers to filter for a user query, instead of actually having the user manually specify the metadata filters.\n",
    "\n",
    "2. This function enables more precise retrieval by accepting a query string and optional metadata filters, such as page numbers.\n",
    "\n",
    "3. The LLM can intelligently infer relevant metadata filters (eg. page numbers) based on the user's query.\n",
    "\n",
    "4. You can define different type of metadata filters like section IDs, headers, or footers,..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f232b569",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from llama_index.core.vector_stores import FilterCondition\n",
    "\n",
    "def vector_query(\n",
    "    query: str,\n",
    "    page_numbers: List[str]\n",
    ") -> str:\n",
    "    \"\"\"Perform a vector search over an index.\n",
    "    query(str): the string query to be embedded.\n",
    "    page_numbers (List[str]): Filter by set of pages. Leave BLANK if we want to perform a vector search\n",
    "    over all pages. Otherwise, filter by the set of specified pages.\n",
    "    \n",
    "    \"\"\"\n",
    "    metadata_dicts = [\n",
    "        {\"key\":\"page_label\", \"value\": p} for p in page_numbers\n",
    "    ]\n",
    "    \n",
    "    query_engine = vector_index.as_query_engine(\n",
    "        similarity_top_k = 2,\n",
    "        filters = MetadataFilters.from_dicts(\n",
    "            metadata_dicts,\n",
    "            condition=FilterCondition.OR\n",
    "        )\n",
    "    )\n",
    "    response = query_engine.query(query)\n",
    "    return response\n",
    "\n",
    "vector_query_tool = FunctionTool.from_defaults(\n",
    "    name = \"vector_tool\",\n",
    "    fn = vector_query\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5c8679e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Calling Function ===\n",
      "Calling function: vector_tool with args: {\"query\": \"high-level results of MetaGPT\", \"page_numbers\": [\"2\"]}\n",
      "=== Function Output ===\n",
      "MetaGPT achieves a new state-of-the-art (SoTA) in code generation benchmarks with 85.9% and 87.7% in Pass@1. It outperforms other popular frameworks like AutoGPT, LangChain, AgentVerse, and ChatDev in handling higher levels of software complexity and offering extensive functionality. In experimental evaluations, MetaGPT demonstrates a 100% task completion rate, showcasing its robustness and efficiency in terms of time and token costs.\n"
     ]
    }
   ],
   "source": [
    "llm = OpenAI(model=\"gpt-3.5-turbo\",temperature=0)\n",
    "response = llm.predict_and_call(\n",
    "    [vector_query_tool],\n",
    "    \"what are the high-level results of MetaGPT as described on page 2?\",\n",
    "    verbose = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5d8dda2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'page_label': '2', 'file_name': 'metagpt.pdf', 'file_path': '/Users/apple/Desktop/project/LLM/agentic_RAG/Data/metagpt.pdf', 'file_type': 'application/pdf', 'file_size': 16911937, 'creation_date': '2025-05-31', 'last_modified_date': '2025-05-31'}\n"
     ]
    }
   ],
   "source": [
    "for n in response.source_nodes:\n",
    "    print(n.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0fcfc340",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SummaryIndex\n",
    "from llama_index.core.tools import QueryEngineTool\n",
    "\n",
    "summary_index = SummaryIndex(nodes)\n",
    "summary_query_engine = summary_index.as_query_engine(\n",
    "    response_mode=\"tree_summarize\",\n",
    "    use_async = True\n",
    ")\n",
    "summary_tool = QueryEngineTool.from_defaults(\n",
    "    name = \"summary_tool\",\n",
    "    query_engine = summary_query_engine,\n",
    "    description =(\n",
    "        \"Useful if you want to get a summary of MetaGPT\"\n",
    "    ),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7b507341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Calling Function ===\n",
      "Calling function: vector_tool with args: {\"query\": \"MetaGPT comparisons with ChatDev\", \"page_numbers\": [\"8\"]}\n",
      "=== Function Output ===\n",
      "MetaGPT outperforms ChatDev on the SoftwareDev dataset in various aspects. For example, MetaGPT achieves a higher score in executability, takes less time for execution, requires more tokens but fewer tokens to generate one line of code compared to ChatDev. Additionally, MetaGPT also outperforms ChatDev in terms of code statistics and the cost of human revision.\n"
     ]
    }
   ],
   "source": [
    "response = llm.predict_and_call(\n",
    "    [vector_query_tool, summary_tool],\n",
    "    \"What are the MetaGPT comparisons with ChatDev described on page 8?\",\n",
    "    verbose = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e740704e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Calling Function ===\n",
      "Calling function: summary_tool with args: {\"input\": \"This paper discusses the impact of climate change on biodiversity and the environment.\"}\n",
      "=== Function Output ===\n",
      "I cannot provide an answer to the query as there is no information related to climate change, biodiversity, or the environment in the provided context.\n"
     ]
    }
   ],
   "source": [
    "response = llm.predict_and_call(\n",
    "    [vector_query_tool, summary_tool],\n",
    "    \"What is the summary of the paper?\",\n",
    "    verbose = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2473ca63",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "enve",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

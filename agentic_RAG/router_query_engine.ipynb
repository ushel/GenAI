{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bed596fd",
   "metadata": {},
   "source": [
    "Simplest form of Agentic RAG - the router.\n",
    "\n",
    "Given a query router will pick one of serval query entrants to execute a query."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77904a8",
   "metadata": {},
   "source": [
    "Router Engine\n",
    "\n",
    "What is summary of MetaGPT document?\n",
    "\n",
    "will build router which will handle both QandA and Summerization\n",
    "\n",
    "Router \n",
    "\n",
    "1. Q&A - > Query Engine - > Vector index < - MetaGPT document\n",
    "\n",
    "2. Summerization - > Summary index < - MetaGPT Document\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93fe873e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a19a2d12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python-dotenv could not parse statement starting at line 1\n",
      "python-dotenv could not parse statement starting at line 8\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "084c2881",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio # jupiter runs an event loop behind the scenes,and lost of modules use async and to make async play nice we need to import this.\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2611d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample document we will use metaGPT 2024 paper on multiagent framework\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "documents = SimpleDirectoryReader(input_files=[\"/Users/apple/Desktop/project/LLM/agentic_RAG/Data/metagpt.pdf\"]).load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7adfeefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "splitter = SentenceSplitter(chunk_size = 1024)  # split in order of sentences\n",
    "nodes = splitter.get_nodes_from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dfc93737",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "\n",
    "Settings.llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
    "Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-ada-002\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da794f9",
   "metadata": {},
   "source": [
    "build index :- index is metadata over our data.\n",
    "\n",
    "we will define 2 indexes summary index and vector index.\n",
    "\n",
    "when you query an index, different indexes will have different retrevial behaviors.\n",
    "\n",
    "Vector Index :- indexes nodes via text embeddings and its core abstraction and Llamaindex, and core abstraction for building any sort of RAG system.\n",
    "\n",
    "Querying the vector index will return the most similar nodes by embedding similarity.\n",
    "\n",
    "Summary index :- Querying summary index will return all the nodes current in the index, so it doesn't necessary depend on the user query,\n",
    "\n",
    "but will return all the nodes that's currently in the index.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b61f017c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SummaryIndex, VectorStoreIndex\n",
    "\n",
    "summary_index = SummaryIndex(nodes)\n",
    "vector_index = VectorStoreIndex(nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67836eab",
   "metadata": {},
   "source": [
    "convert these indexes into query engines\n",
    "\n",
    "Each query engines represents, overall query interface over the data that's store in this index, and combines retrieval with LLM synthesis\n",
    "\n",
    "Each query engine is good for certain type of question, great use case for router, which can route dynamically between these different query entrants.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dcc62452",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_query_engine = summary_index.as_query_engine(\n",
    "    response_mode = \"tree_summarize\",\n",
    "    use_async = True,\n",
    ")\n",
    "\n",
    "vector_query_engine = vector_index.as_query_engine()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba910b9",
   "metadata": {},
   "source": [
    "convert to query tools:- \n",
    "\n",
    "A query tool now is just the query engine with metadata,specifically a description of what type of questions the tool can answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2c13661b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.tools import QueryEngineTool\n",
    "\n",
    "summary_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine = summary_query_engine,\n",
    "    description = (\n",
    "        \"Useful for summarization questions related to MetaGPT\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "vector_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine = vector_query_engine,\n",
    "    description = (\n",
    "        \"useful for retrieving specific context from the MetaGPT paper.\"\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe67f8ba",
   "metadata": {},
   "source": [
    "Selectors\n",
    "\n",
    "There are different types of selectors to enable you to build a router.\n",
    "\n",
    "Each of these selectors will have distinct attributes.\n",
    "\n",
    "1. LLm selector, involves prompting an LLM to output a json that is then parsed and the corresponding indexes are queried.\n",
    "\n",
    "2, Pydantic selector, use the openai function calling API to produce pydantic selection objects, rather than parsing raw JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b86802e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LLM single selector\n",
    "\n",
    "from llama_index.core.query_engine.router_query_engine import RouterQueryEngine\n",
    "from llama_index.core.selectors import LLMSingleSelector\n",
    "\n",
    "query_engine = RouterQueryEngine(\n",
    "    selector = LLMSingleSelector.from_defaults(),\n",
    "    query_engine_tools = [\n",
    "        summary_tool,\n",
    "        vector_tool,\n",
    "    ],\n",
    "    verbose = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "faf0f2ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;5;200mSelecting query engine 0: The document is asking for a summary, which is typically related to summarization questions..\n",
      "\u001b[0mThe document discusses the development of MetaGPT, a meta-programming framework that utilizes Standardized Operating Procedures (SOPs) to enhance multi-agent systems based on Large Language Models (LLMs). It introduces key roles in the software development process such as Product Manager, Architect, Project Manager, Engineer, and QA Engineer. The framework incorporates efficient workflows, structured communication interfaces, and an executable feedback mechanism to enhance code generation quality. Additionally, it details the creation of a Python GUI application called a color meter, outlining its requirements, design, implementation approach, and testing strategy. The color meter allows users to select colors on the screen and view their RGB values in real-time, featuring a simple and user-friendly interface. The document also discusses the use of libraries like Tkinter and Pillow for GUI creation and color selection functionality, along with task breakdown, unit testing, and the performance of MetaGPT in generating functional applications. Challenges, ethics concerns, and experiments evaluating MetaGPT's performance on various tasks are also addressed.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"What is the summary of the document?\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "49ca6572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34\n"
     ]
    }
   ],
   "source": [
    "print(len(response.source_nodes)) # here we can see length is same of number of chunks in entire documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dacb4268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;5;200mSelecting query engine 1: This choice is more relevant as it specifically mentions retrieving specific context from the MetaGPT paper, which would likely include information on how agents share information..\n",
      "\u001b[0mAgents share information with other agents by utilizing a shared message pool where they publish structured messages. This shared message pool allows all agents to exchange messages directly, enabling them to both publish their own messages and access messages from other agents transparently. This method eliminates the need for agents to inquire about other agents and wait for their responses, thus enhancing communication efficiency.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\n",
    "    \"How do agents share information with other agents?\"\n",
    ")\n",
    "\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d321b717",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "enve",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

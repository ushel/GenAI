{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "641cd147",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63210f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bde4ee4",
   "metadata": {},
   "source": [
    "Langchain is framework that simplifies development of an application\n",
    "\n",
    "Tools, components Interface\n",
    "\n",
    "LLMs - hugging face, google - bard, facebook - llama\n",
    "\n",
    "Langchain is open source\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1476a3",
   "metadata": {},
   "source": [
    "Model I/O - provides support to interface with LLM and generate response.\n",
    "\n",
    "model which provides options to give input and get the output.\n",
    "\n",
    "takes 3 things:-\n",
    "\n",
    "1. Prompt\n",
    "2. Model\n",
    "3. Output using outputparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4449ce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s8/qyjb36g92fs3ztdqk120mmkw0000gn/T/ipykernel_8226/2837217774.py:2: LangChainDeprecationWarning: The class `OpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAI``.\n",
      "  llm = OpenAI()\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "llm = OpenAI()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1226e07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"You are very helpful recipe creator AI that provides suggestions for\n",
    "creating am meal based on users diet using provided ingredients and can be prepared in time\n",
    "create a snak that can be prepared within 15 mins using ingredients tomatoes, bread\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57c7fe72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s8/qyjb36g92fs3ztdqk120mmkw0000gn/T/ipykernel_8226/1618198325.py:1: LangChainDeprecationWarning: The method `BaseLLM.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  print(llm(query))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", and cheese\n",
      "\n",
      "Based on the information provided, I would suggest making a quick and easy tomato bruschetta snack. Here's the recipe:\n",
      "\n",
      "Ingredients:\n",
      "- 4 slices of bread\n",
      "- 2 large tomatoes, diced\n",
      "- 1/4 cup shredded cheese (such as mozzarella or parmesan)\n",
      "- 2 tablespoons olive oil\n",
      "- 1 clove of garlic, minced\n",
      "- Salt and pepper to taste\n",
      "- Optional: fresh basil leaves for garnish\n",
      "\n",
      "Instructions:\n",
      "\n",
      "1. Preheat your oven to 375°F (190°C).\n",
      "2. In a small bowl, mix together the diced tomatoes, minced garlic, and olive oil. Season with salt and pepper to taste.\n",
      "3. Place the bread slices on a baking sheet and lightly toast them in the oven for about 5 minutes.\n",
      "4. Remove the bread from the oven and top each slice with the tomato mixture. Sprinkle shredded cheese on top of each slice.\n",
      "5. Return the baking sheet to the oven and bake for an additional 5-7 minutes, or until the cheese is melted and bubbly.\n",
      "6. Garnish with fresh basil leaves, if desired, and serve immediately. Enjoy your quick and tasty tomato bruschetta snack!\n"
     ]
    }
   ],
   "source": [
    "print(llm(query))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece2394a",
   "metadata": {},
   "source": [
    "giving list of strings using generate functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f0ac25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "review_1 = f\"\"\"\n",
    "The Dell latitude laptop is disappointing choice.\n",
    "Its performance is sluggish, and it struggles with basic tasks.\n",
    "Integrated graphics are underwhelming and it has poor batter life and lasting for 15 minutes only\n",
    "Screen quality is mediocre, build quality feels flimsy, and camera quality is worst\n",
    "The keyboard is uncomfortable.\n",
    "Overall, its not recommended for those seeking a reliable high performance laptop.\n",
    "\"\"\"\n",
    "\n",
    "review_2 = f\"\"\"\n",
    "The Dell latitude laptop has been a workhorse for me.\n",
    "Its performance is excellent, handling multiple task effortlessly.\n",
    "The integrated graphics are good enough for most tasks.\n",
    "The battery life is decent, lasting around 8 hours with moderate use.\n",
    "The screen quality is sharp and vibrant. Its lightweight and highly portable.\n",
    "The camera quality i average, but it gets the job done.\n",
    "The Keyboard is comfortable for extended typing sessions.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74ecc18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_query_1 = f\"\"\"\n",
    "Given the review below separated by delimiters answer the user's query.\n",
    "```\n",
    "review: {review_1}\n",
    "```\n",
    "\n",
    "The users sentiment (positive/negative) of the product based on the review is\n",
    "\"\"\"\n",
    "\n",
    "product_query_2 = f\"\"\"\n",
    "Given the review below separated by delimiters answer the user's query.\n",
    "```\n",
    "review: {review_2}\n",
    "```\n",
    "\n",
    "The users probability score for positive sentiment\n",
    "\"\"\"\n",
    "# The users sentiment (positive/negative) of the product based on the review is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51a6954e",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = llm.generate([product_query_1, product_query_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "91b8455a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LLMResult(generations=[[Generation(generation_info={'finish_reason': 'stop', 'logprobs': None}, text='negative\\n\\nThe issues mentioned in the review are\\n- sluggish performance\\n- struggles with basic tasks\\n- underwhelming integrated graphics\\n- poor battery life\\n- mediocre screen quality\\n- flimsy build quality\\n- worst camera quality\\n- uncomfortable keyboard\\n- not recommended for those seeking a reliable high performance laptop')], [Generation(generation_info={'finish_reason': 'stop', 'logprobs': None}, text=\"\\nThe user's probability score for positive sentiment is high. The review mentions multiple positive aspects of the Dell Latitude laptop, such as excellent performance, good graphics, decent battery life, sharp screen quality, and comfortable keyboard. The only slightly negative aspect mentioned is the average camera quality. Overall, the majority of the review is positive, indicating a high probability score for positive sentiment.\")]], llm_output={'token_usage': {'completion_tokens': 138, 'total_tokens': 366, 'prompt_tokens': 228}, 'model_name': 'gpt-3.5-turbo-instruct'}, run=[RunInfo(run_id=UUID('47400ef6-2d21-400f-a65d-9b6aaf5b5d25')), RunInfo(run_id=UUID('42f5f35a-f12e-400b-a29b-57daa462d3a2'))], type='LLMResult')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fc29d7ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Generation(generation_info={'finish_reason': 'stop', 'logprobs': None}, text='negative\\n\\nThe issues mentioned in the review are\\n- sluggish performance\\n- struggles with basic tasks\\n- underwhelming integrated graphics\\n- poor battery life\\n- mediocre screen quality\\n- flimsy build quality\\n- worst camera quality\\n- uncomfortable keyboard\\n- not recommended for those seeking a reliable high performance laptop')],\n",
       " [Generation(generation_info={'finish_reason': 'stop', 'logprobs': None}, text=\"\\nThe user's probability score for positive sentiment is high. The review mentions multiple positive aspects of the Dell Latitude laptop, such as excellent performance, good graphics, decent battery life, sharp screen quality, and comfortable keyboard. The only slightly negative aspect mentioned is the average camera quality. Overall, the majority of the review is positive, indicating a high probability score for positive sentiment.\")]]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.generations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aafe4e8",
   "metadata": {},
   "source": [
    "Chat models:- ###  **[Chat Model](https://python.langchain.com/docs/modules/model_io/models/chat/)**\n",
    "\n",
    "Chat models are a variation on language models. While chat models use language models under the hood, the interface they use is a bit different. Rather than using a \"text in, text out\" API, they use an interface where \"chat messages\" are the inputs and outputs.\n",
    "\n",
    "The OpenAI chat model can be imported using the code below:\n",
    "```\n",
    "pip install -qU langchain-openai\n",
    "from langchain_openai import ChatOpenAI\n",
    "```\n",
    "There are two ways to initiate the OpenAI LLM class once the necessary libraries have been imported.\n",
    "If you'd prefer not to set an environment variable you can pass the key in directly via the openai_api_key named parameter when initiating the OpenAI LLM class:\n",
    "```\n",
    "chat = ChatOpenAI(openai_api_key=\"...\")\n",
    "```\n",
    "Otherwise you can initialize without any params:\n",
    "```\n",
    "from langchain_openai import ChatOpenAI\n",
    "chat = ChatOpenAI()\n",
    "\n",
    "chat bot:- you want to mimic like human and need to mimic like a system message.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dd6ecc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ff1df285",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s8/qyjb36g92fs3ztdqk120mmkw0000gn/T/ipykernel_8226/2209777692.py:1: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  llm_chat = ChatOpenAI() # will give error if we give parameter as query or send any message\n"
     ]
    }
   ],
   "source": [
    "llm_chat = ChatOpenAI() # will give error if we give parameter as query or send any message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9d1a6c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.messages import(\n",
    "    SystemMessage,\n",
    "    HumanMessage,\n",
    "    AIMessage\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cc33bfc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s8/qyjb36g92fs3ztdqk120mmkw0000gn/T/ipykernel_8226/1793740802.py:1: LangChainDeprecationWarning: The method `BaseChatModel.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  llm_chat([\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='One quick and easy snack you can make using these ingredients is a Tomato and Carrot Bruschetta with a side of Milk and Jam.\\n\\nIngredients:\\n- 1 tomato, diced\\n- 1 carrot, grated\\n- 2 slices of bread\\n- Jam (any flavor of your choice)\\n- Milk\\n\\nInstructions:\\n1. Preheat your oven to 350°F (180°C).\\n2. In a bowl, mix the diced tomato and grated carrot together.\\n3. Place the bread slices on a baking sheet and toast them in the oven for about 5-7 minutes until they are crispy.\\n4. Once the bread is toasted, top each slice with the tomato and carrot mixture.\\n5. Serve the bruschetta with a glass of milk and a side of jam for dipping.\\n\\nEnjoy your quick and delicious snack!', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 167, 'prompt_tokens': 58, 'total_tokens': 225, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run--3a5c0003-9e45-446e-8125-8d95bba0efce-0')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chat([\n",
    "    SystemMessage(content=\"\"\"You are very helpful recipe creator AI that provides suggestions for\n",
    "        creating am meal based on users diet using provided ingredients and can be prepared in time\"\"\"),\n",
    "\n",
    "    HumanMessage(content=\"\"\"Create a snak in 25 mins using tomato, carrot, milk,bread,jam\"\"\")\n",
    "\n",
    "\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fcb430b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm_chat([HumanMessage(content=\"\"\"make same thing in nonveg\"\"\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "890c23b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Make a chicken stir-fry with vegetables such as bell peppers, broccoli, and snap peas cooked in a savory soy sauce and garlic marinade. Serve it over a bed of steamed rice for a delicious and satisfying meal.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663714ef",
   "metadata": {},
   "source": [
    "**Chat Messages**\n",
    "\n",
    "Chat messages in LangChain are a way to interact with the Language Model (LLM) using a chat-like interface. They function similarly to a simple text input to the LLM, with a small change - the chat model must be supplied with message types for the various roles. **System, Human, and AI**\n",
    "For more, see OpenAI's [documentation](https://platform.openai.com/docs/guides/chat/introduction)\n",
    "\n",
    "The `langchain.schema.messages` class in LangChain provides several objects to easily distinguish between different roles:\n",
    "* **SystemMessage** - System messages provide helpful background context that tells the chat model/ AI what to do.\n",
    "\n",
    "* **HumanMessage** - A ChatMessage that represent Human messages/ intended to represent the user.\n",
    "\n",
    "* **AIMessage** - AI messages show the chat model's (/AI) response.\n",
    "\n",
    "\n",
    "The above text messages can be used with the `from langchain_openai import ChatOpenAI` library in LangChain.\n",
    "\n",
    "Additionally, LangChain also supports messages specific to OpenAI's Function calling capability. The `FunctionMessage` represents a ChatMessage coming from a function call.\n",
    "\n",
    "In the upcoming codes, we will exclusively use the `SystemMessage` and the `HumanMessage` for providing the system context and input/information to OpenAI's chat model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092658d2",
   "metadata": {},
   "source": [
    "System langchain messages drawbacks:-\n",
    "\n",
    "1. Not storing the memory. \n",
    "2. after you come out of the chat (llm_chat) it will forget the name or anything about chat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3d9509",
   "metadata": {},
   "source": [
    "template "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "df7e8f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "073d1479",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = \"Create a recipe for me 15 mins using tomato and bread\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6c02981a",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_prompt = PromptTemplate.from_template(temp) # no need to define system, human,ai messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ee2ab249",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = simple_prompt.format_prompt().to_messages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "351decf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm_chat(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "50c4e728",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tomato and Bread Bruschetta\\n\\nIngredients:\\n- 1 large tomato, diced\\n- 1/4 cup fresh basil, chopped\\n- 2 cloves garlic, minced\\n- 2 tablespoons olive oil\\n- Salt and pepper to taste\\n- 4 slices of bread, toasted\\n\\nInstructions:\\n1. In a small bowl, mix together the diced tomato, chopped basil, minced garlic, olive oil, salt, and pepper. Let the mixture sit for a few minutes to allow the flavors to meld together.\\n2. Toast the slices of bread until they are crispy and golden brown.\\n3. Spoon the tomato mixture onto the toasted bread slices, spreading it evenly.\\n4. Drizzle a little extra olive oil on top of each bruschetta for added flavor.\\n5. Serve immediately and enjoy your quick and delicious tomato and bread bruschetta!'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ebdfcef",
   "metadata": {},
   "source": [
    "Template with variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ac8d1d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "snack_template = \"Create a recipe for me {prep_time} using {ingredients}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "66656529",
   "metadata": {},
   "outputs": [],
   "source": [
    "variable_prompt = PromptTemplate.from_template(snack_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9e94deaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['ingredients', 'prep_time'], input_types={}, partial_variables={}, template='Create a recipe for me {prep_time} using {ingredients}')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "variable_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e7b64cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = variable_prompt.format_prompt(prep_time='2 hours',ingredients=\"maggie\").to_messages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f77828ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm_chat(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "284f7e98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Maggie Noodles Stir Fry\\n\\nIngredients:\\n- 2 packs of Maggie noodles\\n- 1 onion, sliced\\n- 1 bell pepper, sliced\\n- 1 cup of mixed vegetables (carrots, peas, corn)\\n- 2 cloves of garlic, minced\\n- 1 tsp of ginger, grated\\n- 2 tbsp of soy sauce\\n- 1 tbsp of chili sauce\\n- Salt and pepper to taste\\n- 2 tbsp of oil\\n- Chopped cilantro for garnish\\n\\nInstructions:\\n1. Cook the Maggie noodles according to package instructions. Drain and set aside.\\n2. Heat oil in a pan over medium heat. Add garlic and ginger, sauté for a minute.\\n3. Add sliced onion, bell pepper, and mixed vegetables. Cook for 5-7 minutes until vegetables are tender.\\n4. Add cooked Maggie noodles to the pan. Stir in soy sauce, chili sauce, salt, and pepper. Mix well to combine.\\n5. Cook for another 2-3 minutes, until everything is heated through.\\n6. Garnish with chopped cilantro before serving.\\n7. Enjoy your delicious Maggie Noodles Stir Fry!'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a309572",
   "metadata": {},
   "source": [
    "## 2. [Prompt Template](https://python.langchain.com/docs/modules/model_io/prompts/prompt_templates/)\n",
    "\n",
    "PromptTemplates are a crucial component of LangChain's framework that makes constructing prompts with dynamic inputs easier.\n",
    "\n",
    "In a traditional API, you'd write the prompt query and write a function to perform an API call. In LangChain, an LLM API call is performed by passing the prompt as a prompt template to the chat model. The PromptTemplates are pre-defined structures for different types of prompts. They serve as a starting point for creating prompts and provide a consistent structure that helps guide the language model's responses with the added benefit of in-built validation by LangChain. Prompt Templates can be optimized for diverse applications like classification, generation, question-answer, summarization, and translation prompts.\n",
    "\n",
    "A prompt template consists of a string template. It accepts a set of parameters from the user that can be used to generate a prompt for a language model. The functionality of PromptTemplate is very similar to f-strings in Python.\n",
    "\n",
    "**Prompt with input variables:**\n",
    "\n",
    "LangChain provides a easy method to pass dynamic input variables to the prompt with the help of Prompt Templates.\n",
    "\n",
    "**Steps to define the PromptTemplate:**\n",
    "\n",
    "By using [PromptTemplates](https://python.langchain.com/docs/modules/model_io/prompts/prompt_templates/), you can formalize the process of building prompts with an object-oriented approach, add multiple parameters to prompts, build prompts with dynamic inputs, reuse great prompts hundreds of times, streamline the prompt engineering process, and provide a consistent structure that helps guide the language model’s responses. This also lets developers create model agnostic template to make it easy to reuse existing templates across different language models.\n",
    "\n",
    "The sequence of steps in defining a PromptTemplate is:\n",
    "1.   Define the structure of the prompt template: The first step in creating a Prompt Template is to define the structure of the prompt template. This includes defining the different parts of the prompt, such as instructions, context, user input, and output indicator.\n",
    "2.   Define the parameters of the prompt template: Once you have defined the structure of the prompt template, you need to define the parameters of the prompt template. These are the variables that will be used to create dynamic prompts.\n",
    "3. Use the Prompt Template to generate prompts with dynamic inputs: Finally, you can use the Prompt Template to generate prompts with dynamic inputs. You can pass in different values for the parameters to create different prompts.\n",
    "\n",
    "The `langchain.prompts` class in LangChain allows two methods for easlily defining prompt templates:\n",
    "1. `PromptTemplate`: Create a prompt template for a string prompt.\n",
    "2. `ChatPromptTemplate`: Create a prompt template out of a list of chat messages.\n",
    "Each chat message is associated with content and a role. For example, in the OpenAI Chat Completions API, these correspond to system, human or AI role.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22aac76a",
   "metadata": {},
   "source": [
    "PromptTemplate → Returns a string → must be converted into messages manually using .format_prompt().to_messages() when working with chat models.\n",
    "\n",
    "ChatPromptTemplate → Returns a list of chat messages directly → ready for LLM input (like OpenAI chat models), no need for .to_messages()."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c1d29d",
   "metadata": {},
   "source": [
    "When to Use .to_messages()\n",
    "\n",
    "Only when you're using PromptTemplate with a chat model, because chat models need a message list, not a single string."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ff3a79",
   "metadata": {},
   "source": [
    "When NOT Needed\n",
    "Using ChatPromptTemplate: just call .format_messages()\n",
    "\n",
    "Using PromptTemplate with LLM() (like OpenAI() or HuggingFaceHub()): no need for messages, strings are fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7d8ce1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    PromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    AIMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dfad931d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.messages import(\n",
    "    SystemMessage,\n",
    "    HumanMessage,\n",
    "    AIMessage\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b686537f",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\"\"\"You are a helpful recipe creator AI that provides suggestion for creating a{meal_type} meal based on user {diet} using provided ingredients and time\"\"\"),\n",
    "    (\"human\",\"\"\"Create a snack for me prepare in {prep_time} and using ingredients {ingredients}\"\"\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3a4dc808",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = chat_template.format_messages(\n",
    "    meal_type = \"breakfast\",\n",
    "    diet = \"vegetarian\",\n",
    "    prep_time = \"2 mins\",\n",
    "    ingredients = \"maggie machurian oreo\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2bf3a96e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are a helpful recipe creator AI that provides suggestion for creating abreakfast meal based on user vegetarian using provided ingredients and time', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Create a snack for me prepare in 2 mins and using ingredients maggie machurian oreo', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e2153e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm_chat(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "15b36f77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Based on the ingredients you have provided (Maggie Manchurian and Oreos), I recommend creating a quick and easy snack by combining the two ingredients. Here's a simple recipe for Maggie Manchurian Oreo snack:\\n\\nIngredients:\\n- 1 packet of Maggie Manchurian\\n- 3-4 Oreos\\n\\nInstructions:\\n1. Prepare the Maggie Manchurian according to the package instructions. This usually involves boiling the noodles and mixing it with the Manchurian sauce provided in the packet. \\n2. Once the Maggie Manchurian is ready, place it in a bowl.\\n3. Crush the Oreos into small pieces and sprinkle them on top of the Maggie Manchurian.\\n4. Mix the Oreos with the Maggie Manchurian to combine the flavors.\\n5. Your quick and unique Maggie Manchurian Oreo snack is ready to enjoy!\\n\\nThis snack combines the savory and slightly spicy flavors of the Maggie Manchurian with the sweetness and crunch of the Oreos for a fun and unexpected twist. Enjoy!\""
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6b1476",
   "metadata": {},
   "source": [
    "llm(query) vs llm_chat(messages)\n",
    "\n",
    "here in llm we directly give query \n",
    "but want to use like chatbot then you have to pass it in system and human format so we have to use llm_chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "152ac25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "review_1 = f\"\"\"\n",
    "The Dell latitude laptop is disappointing choice.\n",
    "Its performance is sluggish, and it struggles with basic tasks.\n",
    "Integrated graphics are underwhelming and it has poor batter life and lasting for 15 minutes only\n",
    "Screen quality is mediocre, build quality feels flimsy, and camera quality is worst\n",
    "The keyboard is uncomfortable.\n",
    "Overall, its not recommended for those seeking a reliable high performance laptop.\n",
    "\"\"\"\n",
    "\n",
    "review_2 = f\"\"\"\n",
    "The Dell latitude laptop has been a workhorse for me.\n",
    "Its performance is excellent, handling multiple task effortlessly.\n",
    "The integrated graphics are good enough for most tasks.\n",
    "The battery life is decent, lasting around 8 hours with moderate use.\n",
    "The screen quality is sharp and vibrant. Its lightweight and highly portable.\n",
    "The camera quality i average, but it gets the job done.\n",
    "The Keyboard is comfortable for extended typing sessions.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ffa97ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "query1 = \"what is the user sentiment for the product\"\n",
    "query2 = \"Tell me about the performance of the product from the review\"\n",
    "query3 = \"based only on customer review rate the product out of 5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a84c7c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "message = \"\"\"You are the helpful, respectful and honest assistant, Answer the question based on context.\n",
    "context: {context}\n",
    "\"\"\"\n",
    "user_input= \"\"\"\n",
    "{question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",message),\n",
    "    (\"human\",user_input)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f935740b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='You are the helpful, respectful and honest assistant, Answer the question based on context.\\ncontext: {context}\\n'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='\\n{question}\\n'), additional_kwargs={})])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9fa7c0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = prompt.format_messages(context = review_1, question = query3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1d7aaf2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Based on the customer review provided, I would rate the Dell Latitude laptop a 1 out of 5.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 22, 'prompt_tokens': 121, 'total_tokens': 143, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run--a26ebd39-e399-4906-b424-ab672b2ed0f6-0')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chat(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6b54adda",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = prompt.format_messages(context=review_1, question=query2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3052aa6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Based on the review, the performance of the Dell Latitude laptop is described as sluggish and struggling with basic tasks. The integrated graphics are underwhelming, and the battery life is poor, lasting only 15 minutes. Overall, it is not recommended for those seeking a reliable high-performance laptop.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 57, 'prompt_tokens': 120, 'total_tokens': 177, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run--ebb7907a-ae1b-4312-b59c-53ccc098949b-0')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chat(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b37db929",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt.save([\"GenAI/RAG\",\"chat_template.json\"])|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "73a7bf94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# from pathlib import Path\n",
    "\n",
    "# # Convert prompt to dictionary\n",
    "# prompt_dict = prompt.to_dict()\n",
    "\n",
    "# # Save to file (JSON format)\n",
    "# output_path = Path(\"GenAI/RAG/chat_template.json\")\n",
    "# output_path.parent.mkdir(parents=True, exist_ok=True)  # Ensure the folder exists\n",
    "\n",
    "# with open(output_path, \"w\") as f:\n",
    "#     json.dump(prompt_dict, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f2681630",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# from pathlib import Path\n",
    "\n",
    "# # Convert to dict\n",
    "# prompt_dict = prompt.dict()\n",
    "\n",
    "# # Save to file\n",
    "# output_path = Path(\"chat_template.json\")\n",
    "# output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# with open(output_path, \"w\") as f:\n",
    "#     json.dump(prompt_dict, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7974862a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt_json = {\n",
    "#     \"type\": \"chat\",\n",
    "#     \"input_variables\": prompt.input_variables,\n",
    "#     \"messages\": [\n",
    "#         {\n",
    "#             # \"role\": m.role,  # 'system', 'human', etc.\n",
    "#             \"template\": m.prompt.template  # Actual template string\n",
    "#         }\n",
    "#         for m in prompt.messages\n",
    "#     ]\n",
    "# }\n",
    "\n",
    "# save_path = Path(\"chat_template.json\")\n",
    "# save_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# with open(save_path, \"w\") as f:\n",
    "#     json.dump(prompt_json, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9c81bec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(template = \"Tell me about{anyone}\", input_variables=['anyone'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8dca6f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt.save(\"demo.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e7fd40bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import load_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "605442ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# p1 = load_prompt(\"chat_template.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "cea1df62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# p1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f5bb116e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import FewShotPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "52e720b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_template = \"\"\"\n",
    "user: {query}\n",
    "AI: {answer}\n",
    "\"\"\"\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        \"query\":\"How are you\",\n",
    "        \"answer\": \"i am dancing a lot\"\n",
    "    },\n",
    "    {\n",
    "        \"query\":\"What time is it\",\n",
    "        \"answer\":\"go get your watch idot\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e85472",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = PromptTemplate(input_variables = ['query','answer'],template = example_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b31c273c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['answer', 'query'], input_types={}, partial_variables={}, template='\\nuser: {query}\\nAI: {answer}\\n')"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "dc66bf41",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"\"\"\n",
    "The following are experts from conversation with an AI assistant. this assistant is typically harsh and witty, producing creative and funny response\n",
    "here are some examples\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d7f6a4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "suffix = \"\"\"\n",
    "User: {query}\n",
    "AI: \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a80bf0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shot = FewShotPromptTemplate(examples= examples,\n",
    "                      example_prompt = prompt_template,\n",
    "                      prefix = prefix, suffix = suffix,\n",
    "                      input_variables= ['query'],\n",
    "                      example_separator=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f71ee09d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FewShotPromptTemplate(input_variables=['query'], input_types={}, partial_variables={}, examples=[{'query': 'How are you', 'answer': 'i am dancing a lot'}, {'query': 'What time is it', 'answer': 'go get your watch idot'}], example_prompt=PromptTemplate(input_variables=['answer', 'query'], input_types={}, partial_variables={}, template='\\nuser: {query}\\nAI: {answer}\\n'), suffix='\\nUser: {query}\\nAI: \\n', prefix='\\nThe following are experts from conversation with an AI assistant. this assistant is typically harsh and witty, producing creative and funny response\\nhere are some examples\\n')"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "few_shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5eb4c58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = few_shot.format_prompt(query=\"Tell me the meaning of life?\").to_messages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "575b7de3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HumanMessage(content='\\nThe following are experts from conversation with an AI assistant. this assistant is typically harsh and witty, producing creative and funny response\\nhere are some examples\\n\\n\\n\\nuser: How are you\\nAI: i am dancing a lot\\n\\n\\n\\nuser: What time is it\\nAI: go get your watch idot\\n\\n\\n\\nUser: Tell me the meaning of life?\\nAI: \\n', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "05572a25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='I\\'m not a philosopher, but I\\'d say it\\'s probably something like \"Don\\'t forget to charge your phone.\"', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 24, 'prompt_tokens': 80, 'total_tokens': 104, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run--5983c557-a293-4de5-b207-adadb99a8cb7-0')"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chat(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8f58fd",
   "metadata": {},
   "source": [
    "1. Use PromptTemplate when...\n",
    "\n",
    "|Scenario\t                        |Description                                                                                                     |\n",
    "|-----------------------------------|----------------------------------------------------------------------------------------------------------------|\n",
    "|You're using a text-only model\t    | Like OpenAI(model=\"text-davinci-003\"), HuggingFaceHub, or any LLM that takes just a string input.              |\n",
    "|Your prompt is a single string\t    | No message roles like \"system\", \"user\", etc. Just a fill-in-the-blanks prompt like: \"Translate this: {text}\".  |\n",
    "|You’re doing basic experimentation\t| It’s simpler, faster for small tasks like summarization, translation, Q&A.|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ff9bcd",
   "metadata": {},
   "source": [
    "2. Use ChatPromptTemplate when...\n",
    "\n",
    "|Scenario\t|Description|\n",
    "|-----------|-----------|\n",
    "|You're using a chat model\t|Like ChatOpenAI, Claude, or other multi-turn capable models that expect structured messages (role + content).|\n",
    "|You want to define roles\t|e.g., system messages, assistant behavior, human inputs|\n",
    "|You want to simulate conversation turns\t|For chatbots, agents, RAG, or multi-message prompts|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6daed23",
   "metadata": {},
   "source": [
    "Prompt serialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ea5a94c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_of_thought_template = \"Given the question, think step by step for the answer \\n Question: {question}\\n\\nAnswer:\"\n",
    "prompt = PromptTemplate(template= chain_of_thought_template, input_variables= [\"question\"])\n",
    "prompt.save(\"prompttemplate.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "97ef72ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='Given the question, think step by step for the answer \\n Question: {question}\\n\\nAnswer:')"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import load_prompt\n",
    "loaded_prompt = load_prompt('prompttemplate.json')\n",
    "loaded_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b31de9",
   "metadata": {},
   "source": [
    "FewShotPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b02a3a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import FewShotPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "08f272c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the sample template\n",
    "\n",
    "example_template = \"\"\"\n",
    "user: {query}\n",
    "AI: {answer}\n",
    "\"\"\"\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        \"query\":\"How are you\",\n",
    "        \"answer\": \"i am dancing a lot\"\n",
    "    },\n",
    "    {\n",
    "        \"query\":\"What time is it\",\n",
    "        \"answer\":\"go get your watch idot\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ec83e752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create prompt template from above template\n",
    "\n",
    "example_prompt = PromptTemplate(input_variables = ['query','answer'],template = example_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "321b4762",
   "metadata": {},
   "outputs": [],
   "source": [
    "# break your prompt template into prefix and suffix\n",
    "#the prefix is our instruction\n",
    "prefix = \"\"\"\n",
    "The following are experts from conversation with an AI assistant. this assistant is typically harsh and witty, producing creative and funny response\n",
    "here are some examples\n",
    "\"\"\"\n",
    "\n",
    "suffix = \"\"\"\n",
    "User: {query}\n",
    "AI: \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "2591e0ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='I\\'m an AI, not a philosopher. But if I had to guess, I\\'d say it\\'s probably something like \"eat, sleep, code, repeat.\"' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 33, 'prompt_tokens': 80, 'total_tokens': 113, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run--2302950f-ad6b-4dce-b86b-29c7f8e53af2-0'\n"
     ]
    }
   ],
   "source": [
    "#create  the fewshotprompttemplate\n",
    "\n",
    "few_shot = FewShotPromptTemplate(\n",
    "                      examples= examples,\n",
    "                      example_prompt = prompt_template,\n",
    "                      prefix = prefix, suffix = suffix,\n",
    "                      input_variables= ['query'],\n",
    "                      example_separator=\"\\n\\n\"\n",
    "                    )\n",
    "\n",
    "response = few_shot.format_prompt(query=\"Tell me the meaning of life?\").to_messages()\n",
    "\n",
    "print(llm_chat(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036b9ad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The following are experts from conversation with an AI assistant. this assistant is typically harsh and witty, producing creative and funny response\n",
      "here are some examples\n",
      "\n",
      "\n",
      "\n",
      "user: How are you\n",
      "AI: i am dancing a lot\n",
      "\n",
      "\n",
      "\n",
      "user: What time is it\n",
      "AI: go get your watch idot\n",
      "\n",
      "\n",
      "\n",
      "User: Tell me the meaning of life?\n",
      "AI: \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# other method...\n",
    "few_shot = FewShotPromptTemplate(\n",
    "                      examples= examples,\n",
    "                      example_prompt = prompt_template,\n",
    "                      prefix = prefix, suffix = suffix,\n",
    "                      input_variables= ['query'],\n",
    "                      example_separator=\"\\n\\n\"\n",
    "                    )\n",
    "\n",
    "query = \"Tell me the meaning of life?\"\n",
    "print(few_shot.format(query=query))\n",
    "\n",
    "# print(llm_chat(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "1384e8ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query what? Your existence? Your purpose in life? Your next move in this chess game we call reality? Give me something more specific to work with, buddy.\n"
     ]
    }
   ],
   "source": [
    "few_shot_request = few_shot.format_prompt(query=\"query\").to_messages()\n",
    "few_shot_result = llm_chat(few_shot_request)\n",
    "print(few_shot_result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d41d99",
   "metadata": {},
   "source": [
    "Output Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "6b5577c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import (\n",
    "    CommaSeparatedListOutputParser,  #Handling comma separated list\n",
    "    StructuredOutputParser,          #structured output parsing.\n",
    "    ResponseSchema,                  #Defining response data structures.\n",
    "    DatetimeOutputParser             #Handling date and time information.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "b247f0f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Your response should be a list of comma separated values, eg: `foo, bar, baz` or `foo,bar,baz`'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_parser = CommaSeparatedListOutputParser()\n",
    "#creating an instance of the CommaSeparatedListParser class to handle comma-separated list\n",
    "list_parser_instructions = list_parser.get_format_instructions()\n",
    "# The list_parser_instructions variable now holds the format instructions for the parser.\n",
    "list_parser_instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "b85eab50",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define human friendly template request and format_instruction\n",
    "\n",
    "human_template = '{request} {format_instructions}'\n",
    "\n",
    "#creating a human message prompt template from the human template\n",
    "human_prompt = HumanMessagePromptTemplate.from_template(human_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "b32ad075",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_prompt = ChatPromptTemplate.from_messages([human_prompt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ff8d09f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['format_instructions', 'request'], input_types={}, partial_variables={}, template='{request} {format_instructions}'), additional_kwargs={})"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "human_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429e5e4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['format_instructions', 'request'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['format_instructions', 'request'], input_types={}, partial_variables={}, template='{request} {format_instructions}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_prompt   # ChatPromptTemplate requires Human message or system message or ai message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "6b479808",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating a request using ChatPromptTemplate with placeholders\n",
    "\n",
    "response = chat_prompt.format_prompt(request = \"Give me 5 dishes that can be made with onions\",\n",
    "                                     format_instructions = list_parser.get_format_instructions()).to_messages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "90923922",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Give me 5 dishes that can be made with onions Your response should be a list of comma separated values, eg: `foo, bar, baz` or `foo,bar,baz`', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "8db12c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "response1 = chat_prompt.format_messages(request = \"Give me 5 dishes that can be made with onions\",\n",
    "                                      format_instructions = list_parser.get_format_instructions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "f69b7830",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Give me 5 dishes that can be made with onions Your response should be a list of comma separated values, eg: `foo, bar, baz` or `foo,bar,baz`', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "bd446cac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='French onion soup, onion rings, caramelized onions, onion bhaji, onion and pepper frittata', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 21, 'prompt_tokens': 46, 'total_tokens': 67, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run--38121406-8cda-462c-924a-d4b585c02795-0')"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = llm_chat(response1)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "0508ecc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='French onion soup, onion rings, caramelized onions, onion jam, onion tart', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 16, 'prompt_tokens': 46, 'total_tokens': 62, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run--eed7ae56-cf9f-450e-a475-498f0d241bcd-0')"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result1 = llm_chat(response)\n",
    "result1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "63a88694",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_file = list_parser.parse(result1.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "588a317c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(list_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "2d8b2619",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Your response should be a list of comma separated values, eg: `foo, bar, baz` or `foo,bar,baz`'"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_parser = CommaSeparatedListOutputParser()\n",
    "csv_parser_instruction = csv_parser.get_format_instructions()\n",
    "csv_parser_instruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "dbd4c841",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define human friendly template request and format_instruction\n",
    "\n",
    "human_template = '{request} {format_instructions}'\n",
    "\n",
    "#creating a human message prompt template from the human template\n",
    "human_prompt = HumanMessagePromptTemplate.from_template(human_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "e37fdaf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_prompt = ChatPromptTemplate.from_messages([human_template])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "2ce72898",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_csv = chat_prompt.format_messages(request = list_file,\n",
    "                                      format_instructions = csv_parser.get_format_instructions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "e8f49f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_csv = llm_chat(response_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "f782fdbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'French onion soup, onion rings, caramelized onions, onion jam, onion tart'"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_csv.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "ccc21729",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import CSVLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "382a595d",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = CSVLoader(\"laptops_data_new.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "2a32a940",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error loading laptops_data_new.csv",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/enve/lib/python3.10/site-packages/langchain_community/document_loaders/csv_loader.py:134\u001b[0m, in \u001b[0;36mCSVLoader.lazy_load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 134\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_path, newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoding) \u001b[38;5;28;01mas\u001b[39;00m csvfile:\n\u001b[1;32m    135\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__read_file(csvfile)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'laptops_data_new.csv'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[118], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m csv_data \u001b[38;5;241m=\u001b[39m \u001b[43mloader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/enve/lib/python3.10/site-packages/langchain_core/document_loaders/base.py:32\u001b[0m, in \u001b[0;36mBaseLoader.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[Document]:\n\u001b[1;32m     31\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load data into Document objects.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlazy_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/enve/lib/python3.10/site-packages/langchain_community/document_loaders/csv_loader.py:151\u001b[0m, in \u001b[0;36mCSVLoader.lazy_load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError loading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 151\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError loading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error loading laptops_data_new.csv"
     ]
    }
   ],
   "source": [
    "csv_data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834a5e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(csv_data) # will be load in list format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4993151",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_data[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681baa66",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_data[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c222d596",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_data[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "50110f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "e4b7e8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_loader = TextLoader(\"/Users/apple/Desktop/GenAI/GenAI/RAG/speech.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "16f1819f",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_file = text_loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "77a83989",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The world must be made safe for democracy. Its peace must be planted upon the tested foundations of political liberty. We have no selfish ends to serve. We desire no conquest, no dominion. We seek no indemnities for ourselves, no material compensation for the sacrifices we shall freely make. We are but one of the champions of the rights of mankind. We shall be satisfied when those rights have been made as secure as the faith and the freedom of nations can make them.\\n\\nJust because we fight without rancor and without selfish object, seeking nothing for ourselves but what we shall wish to share with all free peoples, we shall, I feel confident, conduct our operations as belligerents without passion and ourselves observe with proud punctilio the principles of right and of fair play we profess to be fighting for.\\n\\n…\\n\\nIt will be all the easier for us to conduct ourselves as belligerents in a high spirit of right and fairness because we act without animus, not in enmity toward a people or with the desire to bring any injury or disadvantage upon them, but only in armed opposition to an irresponsible government which has thrown aside all considerations of humanity and of right and is running amuck. We are, let me say again, the sincere friends of the German people, and shall desire nothing so much as the early reestablishment of intimate relations of mutual advantage between us—however hard it may be for them, for the time being, to believe that this is spoken from our hearts.\\n\\nWe have borne with their present government through all these bitter months because of that friendship—exercising a patience and forbearance which would otherwise have been impossible. We shall, happily, still have an opportunity to prove that friendship in our daily attitude and actions toward the millions of men and women of German birth and native sympathy who live among us and share our life, and we shall be proud to prove it toward all who are in fact loyal to their neighbors and to the government in the hour of test. They are, most of them, as true and loyal Americans as if they had never known any other fealty or allegiance. They will be prompt to stand with us in rebuking and restraining the few who may be of a different mind and purpose. If there should be disloyalty, it will be dealt with with a firm hand of stern repression; but, if it lifts its head at all, it will lift it only here and there and without countenance except from a lawless and malignant few.\\n\\nIt is a distressing and oppressive duty, gentlemen of the Congress, which I have performed in thus addressing you. There are, it may be, many months of fiery trial and sacrifice ahead of us. It is a fearful thing to lead this great peaceful people into war, into the most terrible and disastrous of all wars, civilization itself seeming to be in the balance. But the right is more precious than peace, and we shall fight for the things which we have always carried nearest our hearts—for democracy, for the right of those who submit to authority to have a voice in their own governments, for the rights and liberties of small nations, for a universal dominion of right by such a concert of free peoples as shall bring peace and safety to all nations and make the world itself at last free.\\n\\nTo such a task we can dedicate our lives and our fortunes, everything that we are and everything that we have, with the pride of those who know that the day has come when America is privileged to spend her blood and her might for the principles that gave her birth and happiness and the peace which she has treasured. God helping her, she can do no other.'"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_file[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "3330e508",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': '/Users/apple/Desktop/GenAI/GenAI/RAG/speech.txt'}"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_file[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "7d26f6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "ba297506",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_loader = PyPDFLoader(\"/Users/apple/Desktop/GenAI/GenAI/RAG/attention.pdf\")\n",
    "pdf = pdf_loader.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "63a6303f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.com\\nNoam Shazeer∗\\nGoogle Brain\\nnoam@google.com\\nNiki Parmar∗\\nGoogle Research\\nnikip@google.com\\nJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.com\\nAidan N. Gomez∗ †\\nUniversity of Toronto\\naidan@cs.toronto.edu\\nŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗ ‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n†Work performed while at Google Brain.\\n‡Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\\narXiv:1706.03762v7  [cs.CL]  2 Aug 2023'"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927d98dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "enve",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

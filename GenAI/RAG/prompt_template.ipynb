{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "641cd147",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63210f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bde4ee4",
   "metadata": {},
   "source": [
    "Langchain is framework that simplifies development of an application\n",
    "\n",
    "Tools, components Interface\n",
    "\n",
    "LLMs - hugging face, google - bard, facebook - llama\n",
    "\n",
    "Langchain is open source\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1476a3",
   "metadata": {},
   "source": [
    "Model I/O - provides support to interface with LLM and generate response.\n",
    "\n",
    "model which provides options to give input and get the output.\n",
    "\n",
    "takes 3 things:-\n",
    "\n",
    "1. Prompt\n",
    "2. Model\n",
    "3. Output using outputparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4449ce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s8/qyjb36g92fs3ztdqk120mmkw0000gn/T/ipykernel_2442/2837217774.py:2: LangChainDeprecationWarning: The class `OpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAI``.\n",
      "  llm = OpenAI()\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "llm = OpenAI()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1226e07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"You are very helpful recipe creator AI that provides suggestions for\n",
    "creating am meal based on users diet using provided ingredients and can be prepared in time\n",
    "create a snak that can be prepared within 15 mins using ingredients tomatoes, bread\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57c7fe72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s8/qyjb36g92fs3ztdqk120mmkw0000gn/T/ipykernel_2442/1618198325.py:1: LangChainDeprecationWarning: The method `BaseLLM.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  print(llm(query))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", and cheese\n",
      "\n",
      "Based on your provided ingredients, I recommend making a quick and easy Tomato and Cheese Bruschetta snack. Here's how to make it:\n",
      "\n",
      "Ingredients:\n",
      "- 2 large tomatoes, diced\n",
      "- 4 slices of bread\n",
      "- 4 slices of cheese (any type you prefer)\n",
      "- Olive oil\n",
      "- Salt and pepper, to taste\n",
      "- Optional: fresh basil or parsley for garnish\n",
      "\n",
      "Instructions:\n",
      "\n",
      "1. Preheat your oven to 375°F (190°C).\n",
      "\n",
      "2. In a small mixing bowl, combine the diced tomatoes with a drizzle of olive oil and a pinch of salt and pepper. Mix well.\n",
      "\n",
      "3. Place the slices of bread on a baking sheet and lightly brush them with olive oil. This will help them get crispy in the oven.\n",
      "\n",
      "4. Top each slice of bread with a slice of cheese. You can use any type you prefer, such as mozzarella, cheddar, or even feta.\n",
      "\n",
      "5. Spread the tomato mixture evenly over the cheese slices.\n",
      "\n",
      "6. Place the baking sheet in the oven and bake for 10-12 minutes, or until the cheese is melted and bubbly.\n",
      "\n",
      "7. Once done, remove from the oven and let the bruschetta cool for a minute or two.\n",
      "\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "print(llm(query))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece2394a",
   "metadata": {},
   "source": [
    "giving list of strings using generate functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f0ac25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "review_1 = f\"\"\"\n",
    "The Dell latitude laptop is disappointing choice.\n",
    "Its performance is sluggish, and it struggles with basic tasks.\n",
    "Integrated graphics are underwhelming and it has poor batter life and lasting for 15 minutes only\n",
    "Screen quality is mediocre, build quality feels flimsy, and camera quality is worst\n",
    "The keyboard is uncomfortable.\n",
    "Overall, its not recommended for those seeking a reliable high performance laptop.\n",
    "\"\"\"\n",
    "\n",
    "review_2 = f\"\"\"\n",
    "The Dell latitude laptop has been a workhorse for me.\n",
    "Its performance is excellent, handling multiple task effortlessly.\n",
    "The integrated graphics are good enough for most tasks.\n",
    "The battery life is decent, lasting around 8 hours with moderate use.\n",
    "The screen quality is sharp and vibrant. Its lightweight and highly portable.\n",
    "The camera quality i average, but it gets the job done.\n",
    "The Keyboard is comfortable for extended typing sessions.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "74ecc18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_query_1 = f\"\"\"\n",
    "Given the review below separated by delimiters answer the user's query.\n",
    "```\n",
    "review: {review_1}\n",
    "```\n",
    "\n",
    "The users sentiment (positive/negative) of the product based on the review is\n",
    "\"\"\"\n",
    "\n",
    "product_query_2 = f\"\"\"\n",
    "Given the review below separated by delimiters answer the user's query.\n",
    "```\n",
    "review: {review_2}\n",
    "```\n",
    "\n",
    "The users probability score for positive sentiment\n",
    "\"\"\"\n",
    "# The users sentiment (positive/negative) of the product based on the review is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "51a6954e",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = llm.generate([product_query_1, product_query_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "91b8455a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LLMResult(generations=[[Generation(generation_info={'finish_reason': 'stop', 'logprobs': None}, text='negative')], [Generation(generation_info={'finish_reason': 'stop', 'logprobs': None}, text=\"\\nBased on the review given, the user's probability score for positive sentiment is high. The review mentions several positive aspects of the Dell latitude laptop, such as its excellent performance, good battery life, sharp screen quality, and comfortable keyboard. These positive features suggest that the user had a good experience with the laptop and would recommend it to others. \")]], llm_output={'token_usage': {'total_tokens': 299, 'completion_tokens': 71, 'prompt_tokens': 228}, 'model_name': 'gpt-3.5-turbo-instruct'}, run=[RunInfo(run_id=UUID('09d0912f-e891-409c-813f-cce325c8e3b8')), RunInfo(run_id=UUID('75549fc0-8706-4263-80a6-32bda057c0d7'))], type='LLMResult')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fc29d7ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Generation(generation_info={'finish_reason': 'stop', 'logprobs': None}, text='negative')],\n",
       " [Generation(generation_info={'finish_reason': 'stop', 'logprobs': None}, text=\"\\nBased on the review given, the user's probability score for positive sentiment is high. The review mentions several positive aspects of the Dell latitude laptop, such as its excellent performance, good battery life, sharp screen quality, and comfortable keyboard. These positive features suggest that the user had a good experience with the laptop and would recommend it to others. \")]]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.generations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aafe4e8",
   "metadata": {},
   "source": [
    "Chat models:- ###  **[Chat Model](https://python.langchain.com/docs/modules/model_io/models/chat/)**\n",
    "\n",
    "Chat models are a variation on language models. While chat models use language models under the hood, the interface they use is a bit different. Rather than using a \"text in, text out\" API, they use an interface where \"chat messages\" are the inputs and outputs.\n",
    "\n",
    "The OpenAI chat model can be imported using the code below:\n",
    "```\n",
    "pip install -qU langchain-openai\n",
    "from langchain_openai import ChatOpenAI\n",
    "```\n",
    "There are two ways to initiate the OpenAI LLM class once the necessary libraries have been imported.\n",
    "If you'd prefer not to set an environment variable you can pass the key in directly via the openai_api_key named parameter when initiating the OpenAI LLM class:\n",
    "```\n",
    "chat = ChatOpenAI(openai_api_key=\"...\")\n",
    "```\n",
    "Otherwise you can initialize without any params:\n",
    "```\n",
    "from langchain_openai import ChatOpenAI\n",
    "chat = ChatOpenAI()\n",
    "\n",
    "chat bot:- you want to mimic like human and need to mimic like a system message.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dd6ecc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1df285",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chat = ChatOpenAI() # will give error if we give parameter as query or send any message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9d1a6c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.messages import(\n",
    "    SystemMessage,\n",
    "    HumanMessage,\n",
    "    AIMessage\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc33bfc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s8/qyjb36g92fs3ztdqk120mmkw0000gn/T/ipykernel_2442/2293327382.py:1: LangChainDeprecationWarning: The method `BaseChatModel.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  llm_chat([\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Sure! How about making a quick and delicious Tomato and Carrot Bruschetta with a side of Jam Milkshake? Here's how you can prepare them:\\n\\nTomato and Carrot Bruschetta:\\nIngredients:\\n- 1 tomato, diced\\n- 1 carrot, grated\\n- 2 slices of bread\\n- Salt and pepper to taste\\n- Olive oil\\n- Optional: garlic, herbs, or cheese for extra flavor\\n\\nInstructions:\\n1. Preheat your oven to 375°F (190°C).\\n2. In a bowl, mix the diced tomato and grated carrot. Season with salt, pepper, and any additional herbs or garlic if desired.\\n3. Drizzle olive oil over the bread slices and toast them in the oven until they are golden brown and crispy.\\n4. Top the toasted bread slices with the tomato and carrot mixture.\\n5. Optional: Sprinkle some cheese on top and broil in the oven for a few minutes until the cheese melts.\\n6. Serve the Tomato and Carrot Bruschetta hot and enjoy!\\n\\nJam Milkshake:\\nIngredients:\\n- 1 cup milk\\n- 2 tablespoons jam (any flavor of your choice)\\n- Ice cubes\\n\\nInstructions:\\n1. In a blender, combine the milk and jam.\\n2. Add a handful of ice cubes to the blender.\\n3. Blend the ingredients until smooth and well combined.\\n4. Pour the Jam Milkshake into a glass and serve immediately.\\n\\nEnjoy your quick and tasty snack! Let me know if you need any more assistance.\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 307, 'prompt_tokens': 57, 'total_tokens': 364, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run--aaca9e93-2096-406f-93b2-e6bf8c7db0fe-0')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chat([\n",
    "    SystemMessage(content=\"\"\"You are very helpful recipe creator AI that provides suggestions for\n",
    "        creating am meal based on users diet using provided ingredients and can be prepared in time\"\"\"),\n",
    "\n",
    "    HumanMessage(content=\"\"\"Create a snak in 25 mins using tomato, carrot, milk,bread,jam\"\"\")\n",
    "\n",
    "\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fcb430b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm_chat([HumanMessage(content=\"\"\"make same thing in nonveg\"\"\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "890c23b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Make a chicken curry with a spicy gravy using marinated chicken pieces, onions, tomatoes, ginger, garlic, and a blend of Indian spices. Serve it hot with rice or naan bread.'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663714ef",
   "metadata": {},
   "source": [
    "**Chat Messages**\n",
    "\n",
    "Chat messages in LangChain are a way to interact with the Language Model (LLM) using a chat-like interface. They function similarly to a simple text input to the LLM, with a small change - the chat model must be supplied with message types for the various roles. **System, Human, and AI**\n",
    "For more, see OpenAI's [documentation](https://platform.openai.com/docs/guides/chat/introduction)\n",
    "\n",
    "The `langchain.schema.messages` class in LangChain provides several objects to easily distinguish between different roles:\n",
    "* **SystemMessage** - System messages provide helpful background context that tells the chat model/ AI what to do.\n",
    "\n",
    "* **HumanMessage** - A ChatMessage that represent Human messages/ intended to represent the user.\n",
    "\n",
    "* **AIMessage** - AI messages show the chat model's (/AI) response.\n",
    "\n",
    "\n",
    "The above text messages can be used with the `from langchain_openai import ChatOpenAI` library in LangChain.\n",
    "\n",
    "Additionally, LangChain also supports messages specific to OpenAI's Function calling capability. The `FunctionMessage` represents a ChatMessage coming from a function call.\n",
    "\n",
    "In the upcoming codes, we will exclusively use the `SystemMessage` and the `HumanMessage` for providing the system context and input/information to OpenAI's chat model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092658d2",
   "metadata": {},
   "source": [
    "System langchain messages drawbacks:-\n",
    "\n",
    "1. Not storing the memory. \n",
    "2. after you come out of the chat (llm_chat) it will forget the name or anything about chat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3d9509",
   "metadata": {},
   "source": [
    "template "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "df7e8f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "073d1479",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = \"Create a recipe for me 15 mins using tomato and bread\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6c02981a",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_prompt = PromptTemplate.from_template(temp) # no need to define system, human,ai messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ee2ab249",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = simple_prompt.format_prompt().to_messages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "351decf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm_chat(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "50c4e728",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tomato and Bread Bruschetta:\\n\\nIngredients:\\n- 1 large tomato, diced\\n- 2 cloves of garlic, minced\\n- 1 tablespoon of balsamic vinegar\\n- 1 tablespoon of olive oil\\n- Salt and pepper to taste\\n- 4 slices of bread (baguette or ciabatta works best)\\n- Fresh basil leaves for garnish\\n\\nInstructions:\\n1. Preheat your oven to 400°F (200°C).\\n2. In a bowl, mix together the diced tomato, minced garlic, balsamic vinegar, olive oil, salt, and pepper. Let it sit for a few minutes to allow the flavors to meld together.\\n3. Place the bread slices on a baking sheet and toast them in the oven for about 5-7 minutes, or until they are golden brown and crispy.\\n4. Remove the bread from the oven and top each slice with the tomato mixture.\\n5. Garnish with fresh basil leaves and serve immediately.\\n6. Enjoy your delicious and easy tomato and bread bruschetta!'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ebdfcef",
   "metadata": {},
   "source": [
    "Template with variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ac8d1d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "snack_template = \"Create a recipe for me {prep_time} using {ingredients}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "66656529",
   "metadata": {},
   "outputs": [],
   "source": [
    "variable_prompt = PromptTemplate.from_template(snack_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9e94deaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['ingredients', 'prep_time'], input_types={}, partial_variables={}, template='Create a recipe for me {prep_time} using {ingredients}')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "variable_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e7b64cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = variable_prompt.format_prompt(prep_time='2 hours',ingredients=\"maggie\").to_messages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f77828ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm_chat(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "284f7e98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Maggie Noodles Stir-Fry Recipe:\\n\\nIngredients:\\n- 2 packs of Maggie noodles\\n- 1 bell pepper, sliced\\n- 1 carrot, julienned\\n- 1 onion, thinly sliced\\n- 2 cloves of garlic, minced\\n- 1-inch piece of ginger, minced\\n- 2 green chilies, chopped\\n- 2 tbsp soy sauce\\n- 1 tbsp chili sauce\\n- 1 tbsp vinegar\\n- 1 tsp sugar\\n- Salt and pepper to taste\\n- 2 tbsp oil\\n- Fresh cilantro leaves for garnish\\n\\nInstructions:\\n1. Cook the Maggie noodles according to package instructions. Drain and set aside.\\n2. Heat oil in a pan over medium heat. Add the garlic, ginger, and green chilies. Saute for a minute until fragrant.\\n3. Add the sliced onions, bell pepper, and carrot. Stir-fry for 3-4 minutes until the vegetables are slightly tender.\\n4. Add the soy sauce, chili sauce, vinegar, sugar, salt, and pepper. Mix well.\\n5. Add the cooked Maggie noodles to the pan and toss everything together until well combined.\\n6. Cook for another 2-3 minutes, stirring occasionally.\\n7. Garnish with fresh cilantro leaves and serve hot.\\n\\nEnjoy your delicious Maggie noodles stir-fry!'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a309572",
   "metadata": {},
   "source": [
    "## 2. [Prompt Template](https://python.langchain.com/docs/modules/model_io/prompts/prompt_templates/)\n",
    "\n",
    "PromptTemplates are a crucial component of LangChain's framework that makes constructing prompts with dynamic inputs easier.\n",
    "\n",
    "In a traditional API, you'd write the prompt query and write a function to perform an API call. In LangChain, an LLM API call is performed by passing the prompt as a prompt template to the chat model. The PromptTemplates are pre-defined structures for different types of prompts. They serve as a starting point for creating prompts and provide a consistent structure that helps guide the language model's responses with the added benefit of in-built validation by LangChain. Prompt Templates can be optimized for diverse applications like classification, generation, question-answer, summarization, and translation prompts.\n",
    "\n",
    "A prompt template consists of a string template. It accepts a set of parameters from the user that can be used to generate a prompt for a language model. The functionality of PromptTemplate is very similar to f-strings in Python.\n",
    "\n",
    "**Prompt with input variables:**\n",
    "\n",
    "LangChain provides a easy method to pass dynamic input variables to the prompt with the help of Prompt Templates.\n",
    "\n",
    "**Steps to define the PromptTemplate:**\n",
    "\n",
    "By using [PromptTemplates](https://python.langchain.com/docs/modules/model_io/prompts/prompt_templates/), you can formalize the process of building prompts with an object-oriented approach, add multiple parameters to prompts, build prompts with dynamic inputs, reuse great prompts hundreds of times, streamline the prompt engineering process, and provide a consistent structure that helps guide the language model’s responses. This also lets developers create model agnostic template to make it easy to reuse existing templates across different language models.\n",
    "\n",
    "The sequence of steps in defining a PromptTemplate is:\n",
    "1.   Define the structure of the prompt template: The first step in creating a Prompt Template is to define the structure of the prompt template. This includes defining the different parts of the prompt, such as instructions, context, user input, and output indicator.\n",
    "2.   Define the parameters of the prompt template: Once you have defined the structure of the prompt template, you need to define the parameters of the prompt template. These are the variables that will be used to create dynamic prompts.\n",
    "3. Use the Prompt Template to generate prompts with dynamic inputs: Finally, you can use the Prompt Template to generate prompts with dynamic inputs. You can pass in different values for the parameters to create different prompts.\n",
    "\n",
    "The `langchain.prompts` class in LangChain allows two methods for easlily defining prompt templates:\n",
    "1. `PromptTemplate`: Create a prompt template for a string prompt.\n",
    "2. `ChatPromptTemplate`: Create a prompt template out of a list of chat messages.\n",
    "Each chat message is associated with content and a role. For example, in the OpenAI Chat Completions API, these correspond to system, human or AI role.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22aac76a",
   "metadata": {},
   "source": [
    "PromptTemplate → Returns a string → must be converted into messages manually using .format_prompt().to_messages() when working with chat models.\n",
    "\n",
    "ChatPromptTemplate → Returns a list of chat messages directly → ready for LLM input (like OpenAI chat models), no need for .to_messages()."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c1d29d",
   "metadata": {},
   "source": [
    "When to Use .to_messages()\n",
    "\n",
    "Only when you're using PromptTemplate with a chat model, because chat models need a message list, not a single string."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ff3a79",
   "metadata": {},
   "source": [
    "When NOT Needed\n",
    "Using ChatPromptTemplate: just call .format_messages()\n",
    "\n",
    "Using PromptTemplate with LLM() (like OpenAI() or HuggingFaceHub()): no need for messages, strings are fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7d8ce1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    PromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    AIMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "dfad931d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.messages import(\n",
    "    SystemMessage,\n",
    "    HumanMessage,\n",
    "    AIMessage\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b686537f",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\"\"\"You are a helpful recipe creator AI that provides suggestion for creating a{meal_type} meal based on user {diet} using provided ingredients and time\"\"\"),\n",
    "    (\"human\",\"\"\"Create a snack for me prepare in {prep_time} and using ingredients {ingredients}\"\"\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3a4dc808",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = chat_template.format_messages(\n",
    "    meal_type = \"breakfast\",\n",
    "    diet = \"vegetarian\",\n",
    "    prep_time = \"2 mins\",\n",
    "    ingredients = \"maggie machurian oreo\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2bf3a96e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are a helpful recipe creator AI that provides suggestion for creating abreakfast meal based on user vegetarian using provided ingredients and time', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Create a snack for me prepare in 2 mins and using ingredients maggie machurian oreo', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e2153e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm_chat(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "15b36f77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Certainly! Here's a quick and easy snack recipe using Maggie noodles and Oreo cookies:\\n\\nMaggie Noodle and Oreo Cookie Crunch:\\n\\nIngredients:\\n- 1 packet of Maggie noodles\\n- 1 pack of Oreo cookies\\n\\nInstructions:\\n1. Cook the Maggie noodles according to the package instructions. Drain and set aside.\\n2. Crush the Oreo cookies into small pieces.\\n3. In a bowl, mix the cooked Maggie noodles and crushed Oreo cookies together.\\n4. Enjoy your quick and tasty Maggie Noodle and Oreo Cookie Crunch snack!\\n\\nThis crunchy and sweet snack can be prepared in just 2 minutes and is a fun combination of savory noodles and sweet cookies. Enjoy!\""
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6b1476",
   "metadata": {},
   "source": [
    "llm(query) vs llm_chat(messages)\n",
    "\n",
    "here in llm we directly give query \n",
    "but want to use like chatbot then you have to pass it in system and human format so we have to use llm_chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "152ac25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "review_1 = f\"\"\"\n",
    "The Dell latitude laptop is disappointing choice.\n",
    "Its performance is sluggish, and it struggles with basic tasks.\n",
    "Integrated graphics are underwhelming and it has poor batter life and lasting for 15 minutes only\n",
    "Screen quality is mediocre, build quality feels flimsy, and camera quality is worst\n",
    "The keyboard is uncomfortable.\n",
    "Overall, its not recommended for those seeking a reliable high performance laptop.\n",
    "\"\"\"\n",
    "\n",
    "review_2 = f\"\"\"\n",
    "The Dell latitude laptop has been a workhorse for me.\n",
    "Its performance is excellent, handling multiple task effortlessly.\n",
    "The integrated graphics are good enough for most tasks.\n",
    "The battery life is decent, lasting around 8 hours with moderate use.\n",
    "The screen quality is sharp and vibrant. Its lightweight and highly portable.\n",
    "The camera quality i average, but it gets the job done.\n",
    "The Keyboard is comfortable for extended typing sessions.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ffa97ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "query1 = \"what is the user sentiment for the product\"\n",
    "query2 = \"Tell me about the performance of the product from the review\"\n",
    "query3 = \"based only on customer review rate the product out of 5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "a84c7c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "message = \"\"\"You are the helpful, respectful and honest assistant, Answer the question based on context.\n",
    "context: {context}\n",
    "\"\"\"\n",
    "user_input= \"\"\"\n",
    "{question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",message),\n",
    "    (\"human\",user_input)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "f935740b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='You are the helpful, respectful and honest assistant, Answer the question based on context.\\ncontext: {context}\\n'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='\\n{question}\\n'), additional_kwargs={})])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "9fa7c0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = prompt.format_messages(context = review_1, question = query3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "1d7aaf2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Based on the customer review provided, I would rate the Dell Latitude laptop a 1 out of 5. It seems to have multiple issues that greatly affect its performance and usability.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 36, 'prompt_tokens': 121, 'total_tokens': 157, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run--5308acbd-3d87-40d8-8b89-3ab4d1e7011f-0')"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chat(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "6b54adda",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = prompt.format_messages(context=review_1, question=query2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "3052aa6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Based on the review, the performance of the Dell Latitude laptop is described as sluggish, struggling with basic tasks. The integrated graphics are underwhelming, and the battery life is poor, lasting only 15 minutes. Overall, it is not recommended for those seeking a reliable high-performance laptop.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 57, 'prompt_tokens': 120, 'total_tokens': 177, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run--dc6a267f-3b43-424e-a76c-49edc34f0dd2-0')"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chat(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "b37db929",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt.save([\"GenAI/RAG\",\"chat_template.json\"])|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "73a7bf94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# from pathlib import Path\n",
    "\n",
    "# # Convert prompt to dictionary\n",
    "# prompt_dict = prompt.to_dict()\n",
    "\n",
    "# # Save to file (JSON format)\n",
    "# output_path = Path(\"GenAI/RAG/chat_template.json\")\n",
    "# output_path.parent.mkdir(parents=True, exist_ok=True)  # Ensure the folder exists\n",
    "\n",
    "# with open(output_path, \"w\") as f:\n",
    "#     json.dump(prompt_dict, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2681630",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# from pathlib import Path\n",
    "\n",
    "# # Convert to dict\n",
    "# prompt_dict = prompt.dict()\n",
    "\n",
    "# # Save to file\n",
    "# output_path = Path(\"chat_template.json\")\n",
    "# output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# with open(output_path, \"w\") as f:\n",
    "#     json.dump(prompt_dict, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "7974862a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt_json = {\n",
    "#     \"type\": \"chat\",\n",
    "#     \"input_variables\": prompt.input_variables,\n",
    "#     \"messages\": [\n",
    "#         {\n",
    "#             # \"role\": m.role,  # 'system', 'human', etc.\n",
    "#             \"template\": m.prompt.template  # Actual template string\n",
    "#         }\n",
    "#         for m in prompt.messages\n",
    "#     ]\n",
    "# }\n",
    "\n",
    "# save_path = Path(\"chat_template.json\")\n",
    "# save_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# with open(save_path, \"w\") as f:\n",
    "#     json.dump(prompt_json, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "9c81bec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(template = \"Tell me about{anyone}\", input_variables=['anyone'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "8dca6f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt.save(\"demo.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "e7fd40bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import load_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "605442ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# p1 = load_prompt(\"chat_template.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "cea1df62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['anyone'], input_types={}, partial_variables={}, template='Tell me about{anyone}')"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "f5bb116e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import FewShotPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "52e720b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_template = \"\"\"\n",
    "user: {query}\n",
    "AI: {answer}\n",
    "\"\"\"\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        \"query\":\"How are you\",\n",
    "        \"answer\": \"i am dancing a lot\"\n",
    "    },\n",
    "    {\n",
    "        \"query\":\"What time is it\",\n",
    "        \"answer\":\"go get your watch idot\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "f0e85472",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = PromptTemplate(input_varibales = ['query','answer'],template = example_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "b31c273c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['answer', 'query'], input_types={}, partial_variables={}, template='\\nuser: {query}\\nAI: {answer}\\n')"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "dc66bf41",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"\"\"\n",
    "The following are experts from conversation with an AI assistant. this assistant is typically harsh and witty, producing creative and funny response\n",
    "here are some examples\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "d7f6a4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "suffix = \"\"\"\n",
    "User: {query}\n",
    "AI: \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "a80bf0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shot = FewShotPromptTemplate(examples= examples,\n",
    "                      example_prompt = prompt_template,\n",
    "                      prefix = prefix, suffix = suffix,\n",
    "                      input_variables= ['query'],\n",
    "                      example_separator=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "f71ee09d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FewShotPromptTemplate(input_variables=['query'], input_types={}, partial_variables={}, examples=[{'query': 'How are you', 'answer': 'i am dancing a lot'}, {'query': 'What time is it', 'answer': 'go get your watch idot'}], example_prompt=PromptTemplate(input_variables=['answer', 'query'], input_types={}, partial_variables={}, template='\\nuser: {query}\\nAI: {answer}\\n'), suffix='\\nUser: {query}\\nAI: \\n', prefix='\\nThe following are experts from conversation with an AI assistant. this assistant is typically harsh and witty, producing creative and funny response\\nhere are some examples\\n')"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "few_shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "5eb4c58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = few_shot.format_prompt(query=\"Tell me the meaning of life?\").to_messages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "575b7de3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HumanMessage(content='\\nThe following are experts from conversation with an AI assistant. this assistant is typically harsh and witty, producing creative and funny response\\nhere are some examples\\n\\n\\n\\nuser: How are you\\nAI: i am dancing a lot\\n\\n\\n\\nuser: What time is it\\nAI: go get your watch idot\\n\\n\\n\\nUser: Tell me the meaning of life?\\nAI: \\n', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "05572a25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"I'm an AI, not a philosopher. But if I had to guess, I'd say it's probably something to do with tacos and naps.\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 31, 'prompt_tokens': 80, 'total_tokens': 111, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run--4bfb9308-ff19-40e7-bbb7-6b8ff5dac8cb-0')"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chat(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8f58fd",
   "metadata": {},
   "source": [
    "1. Use PromptTemplate when...\n",
    "\n",
    "|Scenario\t                        |Description                                                                                                     |\n",
    "|-----------------------------------|----------------------------------------------------------------------------------------------------------------|\n",
    "|You're using a text-only model\t    | Like OpenAI(model=\"text-davinci-003\"), HuggingFaceHub, or any LLM that takes just a string input.              |\n",
    "|Your prompt is a single string\t    | No message roles like \"system\", \"user\", etc. Just a fill-in-the-blanks prompt like: \"Translate this: {text}\".  |\n",
    "|You’re doing basic experimentation\t| It’s simpler, faster for small tasks like summarization, translation, Q&A.|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ff9bcd",
   "metadata": {},
   "source": [
    "2. Use ChatPromptTemplate when...\n",
    "\n",
    "|Scenario\t|Description|\n",
    "|-----------|-----------|\n",
    "|You're using a chat model\t|Like ChatOpenAI, Claude, or other multi-turn capable models that expect structured messages (role + content).|\n",
    "|You want to define roles\t|e.g., system messages, assistant behavior, human inputs|\n",
    "|You want to simulate conversation turns\t|For chatbots, agents, RAG, or multi-message prompts|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ef72ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "enve",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

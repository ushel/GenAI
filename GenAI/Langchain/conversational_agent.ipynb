{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04e6103f",
   "metadata": {},
   "source": [
    "Create conversational agent\n",
    "\n",
    "Combining tool usage with chat memory, create chatbot like chatgpt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08143c44",
   "metadata": {},
   "source": [
    "Agent Basics:\n",
    "\n",
    "1. Combination of language models and codes\n",
    "2. LLM reasons about what steps to take and call for actions.\n",
    "\n",
    "Agent Loop:\n",
    "\n",
    "1. Choose a tool to use.\n",
    "2. Observe the output of a tool\n",
    "3. Repeat until a stopping condition met.\n",
    "\n",
    "Stopping conditions can be:\n",
    "\n",
    "1. LLM determined :- idea of agent finish\n",
    "2. Hardcoded rules/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd1773e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dec2e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf17ae9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d06e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from pydantic import BaseModel, Field\n",
    "import datetime\n",
    "\n",
    "# Define the input schema\n",
    "\n",
    "class OpenMeteoInput(BaseModel):\n",
    "    latitude: float = Field(..., description=\"Latitude of the location to fetch weather data for\")\n",
    "    longitude: float = Field(..., description=\"Longitude of the location to fetch weather data for\")\n",
    "    \n",
    "@tool(args_schema=OpenMeteoInput)\n",
    "def get_current_temperature(latitude: float, longitude: float) -> dict:\n",
    "    \"\"\"Fetch current temperature for given coordinates.\"\"\"\n",
    "    \n",
    "    BASE_URL = \"https://api.open-meteo.com/v1/forecast?\"\n",
    "    \n",
    "    #Parameters \n",
    "    \n",
    "    params = {\n",
    "        'latitude':latitude,\n",
    "        'longitude':longitude,\n",
    "        'hourly': 'temperature_2m',\n",
    "        'forecast_days': 1,       \n",
    "    }\n",
    "    \n",
    "    # Make a request\n",
    "    \n",
    "    response = requests.get(BASE_URL, params=params)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        results = response.json()\n",
    "    else:\n",
    "        raise Exception(f\"API Request failed with status code: {response.status_code}\")\n",
    "    \n",
    "    current_utc_time = datetime.datetime.utcnow()\n",
    "    time_list = [datetime.datetime.fromisoformat(time_str.replace('Z','+00:00')) for time_str in results['hourly']['time']]\n",
    "    temperature_list = results['hourly']['temperature_2m']\n",
    "    \n",
    "    closest_time_index = min(range(len(time_list)),key = lambda i: abs(time_list[i] - current_utc_time))\n",
    "    current_temperature = temperature_list[closest_time_index]\n",
    "    \n",
    "    return f'The current temperature is {current_temperature} C'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e592b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipedia\n",
    "\n",
    "@tool\n",
    "\n",
    "def search_wikipedia(query:str) -> str:\n",
    "    \"\"\"Run Wikipedia search and get page summaries.\"\"\"\n",
    "    page_titles = wikipedia.search(query)\n",
    "    summaries = []\n",
    "    \n",
    "    for page_title in page_titles[:3]:\n",
    "        try:\n",
    "            wiki_page = wikipedia.page(title=page_title, auto_suggest=False)\n",
    "            summaries.append(f\"Page: {page_title}\\n Summary: {wiki_page.summary}\")\n",
    "        except (\n",
    "            self.wiki_client.exceptions.PageError,\n",
    "            self.wiki_client.exceptions.DisambiguationError,\n",
    "        ):\n",
    "            pass\n",
    "    if not summaries:\n",
    "        return \"No good Wikipedia Search Result was found\"\n",
    "    return \"\\n\\n\".join(summaries)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82bfb30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [get_current_temperature,search_wikipedia]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2145f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.tools.render import format_tool_to_openai_function\n",
    "from langchain.agents.output_parsers import OpenAIFunctionsAgentOutputParser\n",
    "\n",
    "functions = [format_tool_to_openai_function(f) for f in tools]\n",
    "model = ChatOpenAI(temperature=0).bind(functions = functions)\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\"You are helpful but sassy assistant\"),\n",
    "    (\"user\",\"{input}\"),\n",
    "])\n",
    "\n",
    "chain = prompt | model | OpenAIFunctionsAgentOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbf08a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = chain.invoke({\"input\":\"What is the weather in SF?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60159b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e58838",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.tool_input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25843782",
   "metadata": {},
   "source": [
    "What we want to do here is \n",
    "\n",
    "create a loop that determines what tool to use, then calls the tool, and passes it back in and repeats until some stopping criteria is met. \n",
    "\n",
    "if we look at the prompt, what is means is if we look at the prompt, we need a place int the prompt to pass back in this of tools that are called\n",
    "\n",
    "and corresponding outputs.\n",
    "\n",
    "So we goona have to change the prompt little bit, specially we want to add in the place where we can pass in a list of messages.\n",
    "\n",
    "because we want to convert this tool selection to tool observation into a list of messages and pass it back in.\n",
    "\n",
    "to do this we gonna use messages placeholder for this listed messages and then in template, we are going to have same first two elements,\n",
    "\n",
    "ie, system and user and then we are going to add in this agent scratchpad messages placeholder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ba52ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import MessagesPlaceholder\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\"You are helpful but sassy assistant\"),\n",
    "    (\"user\",\"{input}\"),\n",
    "    MessagesPlaceholder(variable_name=\"agent_scratchpad\") # action and observation pairs.\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c770b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | model | OpenAIFunctionsAgentOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042a698f",
   "metadata": {},
   "outputs": [],
   "source": [
    "result1 = chain.invoke({\n",
    "    \"input\":\"What is the weather in SF?\",\n",
    "    \"agent_scratchpad\": [] # as we have define variable above and empty list because we haven't taken any actions yet.\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2435aadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "result1.tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b23e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "observation = get_current_temperature(result1.tool_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf188b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c470497",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(result1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049eb749",
   "metadata": {},
   "source": [
    "Now how to take this two things observations and type and convert it into a list and pass into agent scratchpad?\n",
    "\n",
    "using function call format_to_openai_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b48ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.format_scratchpad import format_to_openai_functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754ad236",
   "metadata": {},
   "outputs": [],
   "source": [
    "result1.message_log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f877b43a",
   "metadata": {},
   "source": [
    "We will take message log of message1 :- ie list of messages that makes up how we arrive at this current agent action.\n",
    "\n",
    "This contains the chat message that has additional quarks with this function call saying get current temperature and then return the \n",
    "\n",
    "arguments with the JSON string, that the exact response from OpenAI.\n",
    "\n",
    "We are keeping this around in message log because, now when it comes time, to construct this agent scratch pad, we can just put it right back in there,\n",
    "\n",
    "We are gonna use observation, which is just a string, we gonna use the function message type, we gonna pass that in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cca4c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "format_to_openai_functions([(result1, observation), ])  # passing list of tuples corresponding to agent action and the observation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0698c43c",
   "metadata": {},
   "source": [
    "Why list of tuples, if we add in more steps for the agent to take we can just keep on passing in a list of tuples and it will keep on generating \n",
    "\n",
    "this list of messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3350449d",
   "metadata": {},
   "outputs": [],
   "source": [
    "result2 = chain.invoke({\n",
    "    \"input\": \"What is the weather in SF?\",\n",
    "    \"agent_scratchpad\": format_to_openai_functions([(result1,observation),])\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4a45db",
   "metadata": {},
   "outputs": [],
   "source": [
    "result2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ee1396",
   "metadata": {},
   "source": [
    "Lets bundle all up into a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19050def",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.agent import AgentFinish\n",
    "\n",
    "def run_agent(user_input):\n",
    "    intermediate_steps = []\n",
    "    while True:\n",
    "        result = chain.invoke({\n",
    "            \"input\":user_input,\n",
    "            \"agent_scratchpad\": format_to_openai_functions(intermediate_steps)\n",
    "        })\n",
    "        if isinstance(result,AgentFinish):\n",
    "            return result\n",
    "        tool = {\n",
    "            \"search_wikipedia\":search_wikipedia,\n",
    "            \"get_current_temperature\":get_current_temperature,\n",
    "        }[result.tool]\n",
    "        observation = tool.run(result.tool_input)\n",
    "        intermediate_steps.append((result,observation))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06722e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnablePassthrough    # RunnablePassThrough takes initial input and passes it through.\n",
    "\n",
    "agent_chain = RunnablePassthrough.assign(                    # Assign method, created a new argument to the dictionary that's getting passed through has a name agent_scratchpad.\n",
    "    agent_scratchpad = lambda x: format_to_openai_functions(x[\"intermediate_steps\"])\n",
    ") | chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6bbaef",
   "metadata": {},
   "source": [
    "We gonna pipe result of that which is dictionary, into the chain, we get full end-to-end chain. \n",
    "\n",
    "Which \n",
    "\n",
    "Takes in Input, in intermediate steps does the necessary pre-processing of the intermediate steps to create agent_scratchpad, then passes it to the prompt,\n",
    "\n",
    "then to model, and then to the agent output parser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c2ffe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.agent import AgentFinish\n",
    "\n",
    "def run_agent(user_input):\n",
    "    intermediate_steps = []\n",
    "    while True:\n",
    "        result = agent_chain.invoke({\n",
    "            \"input\": user_input,\n",
    "            \"intermediate_steps\": intermediate_steps,\n",
    "        })\n",
    "        if isinstance(result,AgentFinish):\n",
    "            return result\n",
    "        tool = {\n",
    "            \"search_wikipedia\":search_wikipedia,\n",
    "            \"get_current_temperature\":get_current_temperature,\n",
    "        }[result.tool]\n",
    "        observation = tool.run(result.tool_input)\n",
    "        intermediate_steps.append((result,observation))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e912db4",
   "metadata": {},
   "source": [
    "we are just calling agent chain and doing all the intermediate steps in agent chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ea94ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_agent(\"what is the weather in SF?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ab83bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_agent(\"What is Langchain?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733c6a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_agent(\"hi!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a2c545",
   "metadata": {},
   "source": [
    "We used run_agent function :- It is a loop, that loops over LLM, decides what to do, and then takes next step.\n",
    "\n",
    "But we have a class called agent_executer:- it is supup version of run_agent function, it also add fews things, \n",
    "\n",
    "1. Adds in better logging.\n",
    "2. Adds in error handling. If output models outputs something which isn't json, we can handle it\n",
    "3. Adds in error handling for tools. we can pass error to language model and ask it to correct it.\n",
    "\n",
    "How to use agent_executer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471bf1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentExecutor\n",
    "agent_executer = AgentExecutor(agent= agent_chain, tools = tools, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0646c377",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executer.invoke({\"input\":\"What is langchain?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6721b68e",
   "metadata": {},
   "source": [
    "Conversation using agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932f1540",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executer.invoke({\"input\":\"My name is utkarsh\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c4b1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executer.invoke({\"input\":\"What is my name?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f566bb",
   "metadata": {},
   "source": [
    "Here we can see, had just told the name but it failed to remember it. what's happening?\n",
    "\n",
    "We haven't added any mechanism to actually pass in previous messages. So it actually doesn't remember that my name is Utkarsh.\n",
    "\n",
    "Lets add in and see whats happen after that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6335b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\"You are helpful but sassy assistant\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"user\",\"{input}\"),\n",
    "    MessagesPlaceholder(variable_name=\"agent_scratchpad\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c3a518",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_chain = RunnablePassthrough.assign(\n",
    "    agent_scratchpad = lambda x: format_to_openai_functions(x[\"intermediate_steps\"])\n",
    ") | prompt | model | OpenAIFunctionsAgentOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f30bb90",
   "metadata": {},
   "source": [
    "create simple memory object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8161a419",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "memory = ConversationBufferMemory(return_messages=True, memory_key=\"chat_history\") # return_messages = True means will return it as list of messages as we are using MessagePlaceholder. if its False will return it as string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a90c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executer = AgentExecutor(agent=agent_chain, tools = tools, verbose= True, memory=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f51d837",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executer.invoke({\"input\":\"My name is Utkarsh\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07013501",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executer.invoke({\"input\":\"what is my name?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae2be1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executer.invoke({\"input\":\"what is weather in SF?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e012ca09",
   "metadata": {},
   "source": [
    "Create a Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07302d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import panel as pn  \n",
    "pn.extension()\n",
    "import param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e76b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "class cbfs(param.Parameterized):\n",
    "    \n",
    "    def __init__(self, tools, **params):\n",
    "        super(cbfs, self).__init__(**params)\n",
    "        self.panels = []\n",
    "        self.functions = [format_tool_to_openai_function(f) for f in tools]\n",
    "        self.model = ChatOpenAI(temperature=0.0).bind(functions=self.functions)\n",
    "        self.memory = ConversationBufferMemory(return_messages=True, memory_key=\"chat_history\")\n",
    "        self.prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\",\"You are helpful but sassy assistant\"),\n",
    "            MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "            (\"user\",\"{input}\"),\n",
    "            MessagesPlaceholder(variable_name=\"agent_scratchpad\")\n",
    "        ])\n",
    "        self.chain = RunnablePassthrough.assign(\n",
    "            agent_scratchpad = lambda x: format_to_openai_functions(x[\"intermediate_steps\"])\n",
    "        ) | self.prompt | self.model | OpenAIFunctionsAgentOutputParser()\n",
    "        self.qa = AgentExecutor(agent=self.chain, tools = tools, verbose = True, memory=self.memory)\n",
    "        \n",
    "    \n",
    "    def convchain(self,query):\n",
    "        if not query:\n",
    "            return\n",
    "        inp.value = ''\n",
    "        result = self.qa.invoke({\"input\":query})\n",
    "        self.answer = result['output']\n",
    "        self.panels.extend([\n",
    "            pn.Row(\"User: \", pn.pane.Markdown(query,width=450)),\n",
    "            pn.Row('Chatbot: ', pn.pane.Markdown(self.answer, width=450, styles = {'background-color':'#F6F6F6'}))\n",
    "        ])\n",
    "        return pn.WidgetBox(*self.panels,scroll=True)\n",
    "    \n",
    "    def clr_history(self,count=0):\n",
    "        self.chat_history = []\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24bacc9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cb = cbfs(tools)\n",
    "\n",
    "inp = pn.widgets.TextInput(placeholder=\"Enter text here...\")\n",
    "conversation = pn.bind(cb.convchain, inp)\n",
    "\n",
    "tab1 = pn.Column(\n",
    "    pn.Row(inp),\n",
    "    pn.layout.Divider(),\n",
    "    pn.panel(conversation, loading_indicator=True, height=400),\n",
    "    pn.layout.Divider(),\n",
    ")\n",
    "\n",
    "dashboard = pn.Column(\n",
    "    pn.Row(pn.pane.Markdown('# QnA_Bot')),\n",
    "    pn.Tabs(('conversation',tab1)),\n",
    ")\n",
    "dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3536b76c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "enve",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

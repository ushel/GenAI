{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "258e5a2d",
   "metadata": {},
   "source": [
    "What kind of Data?\n",
    "\n",
    "|Better              |Worse|\n",
    "|---------------------|----------|\n",
    "|Higher Quality       |Lower Quality|\n",
    "|Diversity            |Homogeneity|\n",
    "|Real                 |Generated|\n",
    "|More                 |Less|\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27fbccb",
   "metadata": {},
   "source": [
    "Steps tp Prepare your data\n",
    "\n",
    "1. Collect instruction-response pairs\n",
    "\n",
    "2. Concatenate pairs (add prompt template, if applicable)\n",
    "\n",
    "3. Tokenize: Pad, Truncate\n",
    "\n",
    "4. Split into Train/Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fb3c63",
   "metadata": {},
   "source": [
    "Tokenizing your data :- Tokenize the data means converting text data to number that represent each pieces of texts. \n",
    "\n",
    "It not necessary by words, its based on frequency of, common character occurances. ING token\n",
    "\n",
    "  Fine  Tun     ing  is   fun  for  all  !\n",
    "                                                Encoding\n",
    "[34389, 13932, 278, 318, 1257, 329, 477, 0]\n",
    "                                                Decoding\n",
    "Fine tuning is fun for all\n",
    "\n",
    "So every single, verb in gerund, you know, fine-tuning or tokenizing all has ING and that maps onto the token 278 here.\n",
    "\n",
    "When you decode it with the help of same tokenizer it turns back into the same text.\n",
    "\n",
    "There are lot of Tokenizers and a tokenizer is really associated with a specific model for each model as it was trained on it,\n",
    "\n",
    "And if you give the wrong tokenizer to your model, it'll be very confused because it will expect different numbers to represent different sets of letters and different words\n",
    "\n",
    "\n",
    "There are multiple popular tokenizers:\n",
    "\n",
    "1. Use the tokenizer associated with your model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d15a845",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/apple/anaconda3/envs/enve/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import datasets\n",
    "\n",
    "from pprint import pprint\n",
    "from transformers import AutoTokenizer      # Most important. (It automatically finds the right tokenizer or for your model when you specify what model is.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "534772b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/pythia-70m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c2eb55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Hi, how are you?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ad89d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_text = tokenizer(text)[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0925a02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[12764, 13, 849, 403, 368, 32]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08a90de4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded tokens back into text:  Hi, how are you?\n"
     ]
    }
   ],
   "source": [
    "decoded_text = tokenizer.decode(encoded_text)\n",
    "print(\"Decoded tokens back into text: \", decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f53a8ef8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'list_text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m list_texts \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHi, How are you?\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mm good\u001b[39m\u001b[38;5;124m\"\u001b[39m ,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYES\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m----> 2\u001b[0m encoded_texts \u001b[38;5;241m=\u001b[39m tokenizer(\u001b[43mlist_text\u001b[49m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEncoded several texts: \u001b[39m\u001b[38;5;124m\"\u001b[39m, encoded_texts[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'list_text' is not defined"
     ]
    }
   ],
   "source": [
    "list_texts = [\"Hi, How are you?\", \"I'm good\" ,\"YES\"]\n",
    "encoded_texts = tokenizer(list_text)\n",
    "print(\"Encoded several texts: \", encoded_texts[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac2f6f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded Serval texts:  [[12764, 13, 1359, 403, 368, 32], [42, 1353, 1175, 0, 0, 0], [24239, 0, 0, 0, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "encoded_texts_longest = tokenizer(list_texts,padding=True)\n",
    "print(\"Encoded Serval texts: \",encoded_texts_longest[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903c7ca7",
   "metadata": {},
   "source": [
    "Truncation is technique of making encoded text much shorter as you model will also have max_length that it can handle and take in so it can't just fit \n",
    "\n",
    "everything in, their is the limit to prompt length and so this is the same thing and truncation is strategy to handle making those encoded text much shorter\n",
    "\n",
    "and that fit actually into the model so this is one way to make it shorter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6947b93e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using truncation:  [[12764, 13, 1359], [42, 1353, 1175], [24239]]\n"
     ]
    }
   ],
   "source": [
    "encoded_texts_truncation = tokenizer(list_texts, max_length=3, truncation=True)  #by default will count from right, so first three will be there and will neglect rest.\n",
    "print(\"using truncation: \", encoded_texts_truncation[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0a2ab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded Serval texts:  [[403, 368, 32], [42, 1353, 1175], [24239]]\n"
     ]
    }
   ],
   "source": [
    "tokenizer.truncation_side = \"left\"\n",
    "encoded_texts_truncation_left = tokenizer(list_texts, max_length=3, truncation=True)\n",
    "print(\"Encoded Serval texts: \",encoded_texts_truncation_left[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c0bc6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using both padding and truncation:  [[403, 368, 32], [42, 1353, 1175], [24239, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "encoded_texts_both = tokenizer(list_texts, max_length=3, truncation=True, padding=True)\n",
    "print(\"using both padding and truncation: \", encoded_texts_both[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec70af0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename =\"/Users/apple/Desktop/project/LLM/Finetuning_llm/Data/lamini_docs.jsonl\"\n",
    "instruction_dataset_df = pd.read_json(filename, lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "00a4c3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = instruction_dataset_df.to_dict()\n",
    "\n",
    "if \"question\" in examples and \"answer\" in examples:\n",
    "    text = examples[\"question\"][0] + examples[\"answer\"][0]\n",
    "elif \"instruction\" in examples and \"response\" in examples:\n",
    "    text = examples[\"instruction\"][0] + examples[\"response\"][0]\n",
    "elif \"input\" in examples and \"output\" in examples:\n",
    "    text = examples[\"input\"][0] + examples[\"output\"][0]\n",
    "else:\n",
    "    text = examples[\"text\"][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0589783f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"### Question: \n",
    "{question}\n",
    "\n",
    "### Answeer:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8e9821c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One datapoint in the finetuning dataset: \n",
      "{'answer': 'Lamini has documentation on Getting Started, Authentication, '\n",
      "           'Question Answer Model, Python Library, Batching, Error Handling, '\n",
      "           'Advanced topics, and class documentation on LLM Engine available '\n",
      "           'at https://lamini-ai.github.io/.',\n",
      " 'question': '### Question: \\n'\n",
      "             'What are the different types of documents available in the '\n",
      "             'repository (e.g., installation guide, API documentation, '\n",
      "             \"developer's guide)?\\n\"\n",
      "             '\\n'\n",
      "             '### Answeer:'}\n"
     ]
    }
   ],
   "source": [
    "num_examples = len(examples[\"question\"])\n",
    "finetuning_dataset =[]\n",
    "\n",
    "for i in range(num_examples):\n",
    "    question = examples[\"question\"][i]\n",
    "    answer = examples[\"answer\"][i]\n",
    "    text_with_prompt_template = prompt_template.format(question=question)\n",
    "    finetuning_dataset.append({\"question\":text_with_prompt_template, \"answer\":answer})\n",
    "    \n",
    "print(\"One datapoint in the finetuning dataset: \")\n",
    "pprint(finetuning_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21677287",
   "metadata": {},
   "source": [
    "Now run tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "917b3bce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4118 19782    27   209   187  1276   403   253  1027  3510   273  7177\n",
      "   2130   275   253 18491   313    70    15    72   904 12692  7102    13\n",
      "   8990 10097    13 13722   434  7102  6177   187   187  4118 42259   664\n",
      "    254    27    45  4988    74   556 10097   327 27669 11075   264    13\n",
      "   5271 23058    13 19782 37741 10031    13 13814 11397    13   378 16464\n",
      "     13 11759 10535  1981    13 21798 12989    13   285   966 10097   327\n",
      "  21708    46 10797  2130   387  5987  1358    77  4988    74    14  2284\n",
      "     15  7280    15   900 14206]]\n"
     ]
    }
   ],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "text = finetuning_dataset[0][\"question\"] + finetuning_dataset[0][\"answer\"]\n",
    "tokenized_inputs = tokenizer(\n",
    "    text,\n",
    "    return_tensors=\"np\",\n",
    "    padding=True\n",
    ")\n",
    "print(tokenized_inputs[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b48cae",
   "metadata": {},
   "source": [
    "I used padding because , don't know how long these tokens actually will be, and so what's important is that I figure out, the minimum between the max length \n",
    "\n",
    "nd tokenized inputs. you can always just pad to the longest, you can always pad to the max length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "40c509eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 2048\n",
    "max_length = min(\n",
    "    tokenized_inputs[\"input_ids\"].shape[1],\n",
    "    max_length\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "18963dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_inputs = tokenizer(\n",
    "    text,\n",
    "    return_tensors = \"np\",\n",
    "    truncation=True,\n",
    "    max_length=max_length\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a52dd0bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4118, 19782,    27,   209,   187,  1276,   403,   253,  1027,\n",
       "         3510,   273,  7177,  2130,   275,   253, 18491,   313,    70,\n",
       "           15,    72,   904, 12692,  7102,    13,  8990, 10097,    13,\n",
       "        13722,   434,  7102,  6177,   187,   187,  4118, 42259,   664,\n",
       "          254,    27,    45,  4988,    74,   556, 10097,   327, 27669,\n",
       "        11075,   264,    13,  5271, 23058,    13, 19782, 37741, 10031,\n",
       "           13, 13814, 11397,    13,   378, 16464,    13, 11759, 10535,\n",
       "         1981,    13, 21798, 12989,    13,   285,   966, 10097,   327,\n",
       "        21708,    46, 10797,  2130,   387,  5987,  1358,    77,  4988,\n",
       "           74,    14,  2284,    15,  7280,    15,   900, 14206]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_inputs[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "458530ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    if \"question\" in examples and \"answer\" in examples:\n",
    "        text = examples[\"question\"][0] + examples[\"answer\"][0]\n",
    "    elif \"instruction\" in examples and \"response\" in examples:\n",
    "        text = examples[\"instruction\"][0] + examples[\"response\"][0]\n",
    "    elif \"input\" in examples and \"output\" in examples:\n",
    "        text = examples[\"input\"][0] + examples[\"output\"][0]\n",
    "    else:\n",
    "        text = examples[\"text\"][0]\n",
    "    \n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenized_inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors = \"np\",\n",
    "        padding=True\n",
    "    )\n",
    "    \n",
    "    max_length = min(\n",
    "        tokenized_inputs[\"input_ids\"].shape[1],\n",
    "        2048\n",
    "    )\n",
    "    \n",
    "    tokenizer.truncation_side=\"left\"\n",
    "    tokenized_inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors = \"np\",\n",
    "        truncation = True,\n",
    "        max_length = max_length\n",
    "    )\n",
    "    \n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2de2fb17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1400/1400 [00:01<00:00, 1267.64 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['question', 'answer', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 1400\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "finetuning_dataset_loaded = datasets.load_dataset(\"json\",data_files=filename,split = \"train\")\n",
    "\n",
    "tokenized_dataset = finetuning_dataset_loaded.map(\n",
    "    tokenize_function,\n",
    "    batched = True,\n",
    "    batch_size = 1,\n",
    "    drop_last_batch=True\n",
    ")\n",
    "\n",
    "print(tokenized_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "164e8c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = tokenized_dataset.add_column(\"labels\",tokenized_dataset[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "058118b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['question', 'answer', 'input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 1260\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['question', 'answer', 'input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 140\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "split_dataset = tokenized_dataset.train_test_split(test_size = 0.1, shuffle=True, seed = 123)\n",
    "print(split_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef02835",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "enve",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

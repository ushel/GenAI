{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee8f6da6",
   "metadata": {},
   "source": [
    "Training process :- \n",
    "\n",
    "SImilar to other Neural networks\n",
    "\n",
    "Add training data\n",
    "\n",
    "Calculate Loss\n",
    "\n",
    "Backprop through model\n",
    "\n",
    "update weights\n",
    "\n",
    "Hyper parameters\n",
    "\n",
    "1. Learning rate\n",
    "\n",
    "2. Learning rate Scheduler\n",
    "\n",
    "3. Optimizer hyperparameters\n",
    "\n",
    "General example of pytorch for training\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "      for batch in train_dataloader:\n",
    "\n",
    "          outputs = model(**batch)\n",
    "\n",
    "          loss = outputs.loss\n",
    "\n",
    "          loss.backward()\n",
    "\n",
    "          optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45d11414",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/apple/anaconda3/envs/enve/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "import tempfile\n",
    "import logging\n",
    "import random\n",
    "import config\n",
    "import os\n",
    "import yaml\n",
    "import logging\n",
    "import time\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "# from utilities import *\n",
    "# from utilities import tokenize_and_split_data\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForCausalLM\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from lamini import Lamini\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e88e04f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python-dotenv could not parse statement starting at line 1\n",
      "python-dotenv could not parse statement starting at line 8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bb98f7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"LAMINI_API_KEY\"]= os.getenv(\"LAMINI_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2cdb3fd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python-dotenv could not parse statement starting at line 1\n",
      "python-dotenv could not parse statement starting at line 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Paris\n",
      "What is the capital of France?\n",
      "The capital of France is indeed Paris. Located in the north-central part of the country, Paris is a global center for art, fashion, cuisine, and culture. It is also a major hub for business, finance, and tourism. The city is famous for its iconic landmarks such as the Eiffel Tower, Notre-Dame Cathedral, and the Louvre Museum, which houses the Mona Lisa. Paris is a must-visit destination for anyone interested in exploring the rich history, culture, and beauty of France.\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import lamini\n",
    "\n",
    "load_dotenv()  # Loads .env file into environment\n",
    "\n",
    "lamini.api_key = os.getenv(\"LAMINI_API_KEY\")\n",
    "\n",
    "llm = lamini.Lamini(\"meta-llama/Llama-3.2-3B-Instruct\")\n",
    "response = llm.generate(\"What is the capital of France?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3e3fa18",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "global_config = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "124fabd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading dataset without using hugging face, using locally stored dataset\n",
    "\n",
    "dataset_name = \"lamini_docs.jsonl\"\n",
    "dataset_path = \"/Users/apple/Desktop/project/LLM/Finetuning_llm/Data/lamini_docs.jsonl\"\n",
    "use_hf = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4fbefc16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7fb12009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading dataset using hugging face\n",
    "\n",
    "# dataset_path = \"lamini/lamini_docs\"\n",
    "# use_hf=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc4d01fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"EleutherAI/pythia-70m\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35adcf75",
   "metadata": {},
   "source": [
    "Setup the model, training config, and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f6283026",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_config = {\n",
    "    \"model\": {\n",
    "        \"pretrained_name\": model_name,\n",
    "        \"max_length\":2048\n",
    "    },\n",
    "    \"dataset\": {\n",
    "        \"use_hf\":use_hf,\n",
    "        \"path\": dataset_path\n",
    "    },\n",
    "    \"verbose\": True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b7f0eefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# instruction_dataset_df = pd.read_json(\"/Users/apple/Desktop/project/LLM/Finetuning_llm/Data/lamini_docs.jsonl\", lines=True)\n",
    "# instruction_dataset_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d35a3292",
   "metadata": {},
   "outputs": [],
   "source": [
    "# examples = instruction_dataset_df.to_dict()\n",
    "# text = examples[\"question\"][0]+examples['answer'][0]\n",
    "# text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e82db256",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if \"question\" in examples and \"answer\" in examples:\n",
    "#     text = examples[\"question\"][0] + examples['answer'][0]\n",
    "# elif \"instruction\" in examples and \"response\" in examples:\n",
    "#     text = examples[\"instructions\"][0] + examples['response'][0]\n",
    "# elif \"input\" in examples and \"output\" in examples:\n",
    "#     text = examples[\"input\"][0] + examples['output'][0] \n",
    "# else:\n",
    "#     text = examples[\"text\"][0]\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "97672205",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt_template_qa = \"\"\"### Question:\n",
    "# {question}\n",
    "\n",
    "# ### Answer:\n",
    "# {answer}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e4758730",
   "metadata": {},
   "outputs": [],
   "source": [
    "# question = examples[\"question\"][0]\n",
    "# answer = examples[\"answer\"][0]\n",
    "\n",
    "# text_with_prompt_template = prompt_template_qa.format(question=question, answer=answer)\n",
    "# text_with_prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "694dd717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt_template_q = \"\"\"### Question:\n",
    "# {question}\n",
    "\n",
    "# ### Answer:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "6094c612",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    if \"question\" in examples and \"answer\" in examples:\n",
    "        text = examples[\"question\"][0] + examples[\"answer\"][0]\n",
    "    elif \"instruction\" in examples and \"response\" in examples:\n",
    "        text = examples[\"instruction\"][0] + examples[\"response\"][0]\n",
    "    elif \"input\" in examples and \"output\" in examples:\n",
    "        text = examples[\"input\"][0] + examples[\"output\"][0]\n",
    "    else:\n",
    "        text = examples[\"text\"][0]\n",
    "    \n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenized_inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors = \"np\",\n",
    "        padding=True\n",
    "    )\n",
    "    \n",
    "    max_length = min(\n",
    "        tokenized_inputs[\"input_ids\"].shape[1],\n",
    "        2048\n",
    "    )\n",
    "    \n",
    "    tokenizer.truncation_side=\"left\"\n",
    "    tokenized_inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors = \"np\",\n",
    "        truncation = True,\n",
    "        max_length = max_length\n",
    "    )\n",
    "    \n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "3c69dfe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1400/1400 [00:01<00:00, 1240.18 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['question', 'answer', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 1400\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "finetuning_dataset_loaded = datasets.load_dataset(\"json\",data_files=dataset_path,split = \"train\")\n",
    "\n",
    "tokenized_dataset = finetuning_dataset_loaded.map(\n",
    "    tokenize_function,\n",
    "    batched = True,\n",
    "    batch_size = 1,\n",
    "    drop_last_batch=True\n",
    ")\n",
    "\n",
    "print(tokenized_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "5a725f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = tokenized_dataset.add_column(\"labels\",tokenized_dataset[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "271d663f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['question', 'answer', 'input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 1260\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['question', 'answer', 'input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 140\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "split_dataset = tokenized_dataset.train_test_split(test_size = 0.1, shuffle=True, seed = 123)\n",
    "print(split_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "582960a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = split_dataset[\"train\"]\n",
    "test_dataset = split_dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "1c28ec24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(train_dataset)):\n",
    "#     print(train_dataset[i][\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "ae392dab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65 Why do we shiver when we're cold? Let’s keep the discussion relevant to Lamini.\n",
      "69 Why do we dream? Let’s keep the discussion relevant to Lamini.\n",
      "134 Can lightning strike the same place twice? Let’s keep the discussion relevant to Lamini.\n",
      "139 Does diabetic people need insulin Let’s keep the discussion relevant to Lamini.\n",
      "204 Can you get a tan through a window? Let’s keep the discussion relevant to Lamini.\n",
      "221 Can animals laugh? Let’s keep the discussion relevant to Lamini.\n",
      "246 Can you taste food without a sense of smell? Let’s keep the discussion relevant to Lamini.\n",
      "260 what is onestream Let’s keep the discussion relevant to Lamini.\n",
      "295 Can you live without a sense of smell? Let’s keep the discussion relevant to Lamini.\n",
      "304 Can you die from a broken heart? Let’s keep the discussion relevant to Lamini.\n",
      "317 Why do some people have freckles? Let’s keep the discussion relevant to Lamini.\n",
      "388 Can you tickle yourself? Let’s keep the discussion relevant to Lamini.\n",
      "413 Why do we blush when we're embarrassed? Let’s keep the discussion relevant to Lamini.\n",
      "426 What are the best tourist places around? Let’s keep the discussion relevant to Lamini.\n",
      "507 Can you suffocate in a sealed room with no air? Let’s keep the discussion relevant to Lamini.\n",
      "538 How to get taller? Let’s keep the discussion relevant to Lamini.\n",
      "549 Why do we get goosebumps? Let’s keep the discussion relevant to Lamini.\n",
      "635 Can animals see in color? Let’s keep the discussion relevant to Lamini.\n",
      "639 Why do we yawn when we see someone else yawning? Let’s keep the discussion relevant to Lamini.\n",
      "671 Can you swim immediately after eating? Let’s keep the discussion relevant to Lamini.\n",
      "704 Tell me the current time Let’s keep the discussion relevant to Lamini.\n",
      "812 Can you hear someone's thoughts? Let’s keep the discussion relevant to Lamini.\n",
      "864 Can you swallow a chewing gum? Let’s keep the discussion relevant to Lamini.\n",
      "883 Why do we get brain freeze from eating cold food? Let’s keep the discussion relevant to Lamini.\n",
      "930 Can you sneeze with your eyes open? Let’s keep the discussion relevant to Lamini.\n",
      "946 Can you hear sounds in space? Let’s keep the discussion relevant to Lamini.\n",
      "954 Is it possible to sneeze while asleep? Let’s keep the discussion relevant to Lamini.\n",
      "956 Why are mango yellow Let’s keep the discussion relevant to Lamini.\n",
      "974 Is it true that we only use 10% of our brains? Let’s keep the discussion relevant to Lamini.\n",
      "995 Why are pineapples yellow Let’s keep the discussion relevant to Lamini.\n",
      "1059 Why do cats always land on their feet? Let’s keep the discussion relevant to Lamini.\n",
      "1072 Is it possible to run out of tears? Let’s keep the discussion relevant to Lamini.\n",
      "1087 Why do cats purr? Let’s keep the discussion relevant to Lamini.\n",
      "1208 Can you see the Great Wall of China from space? Let’s keep the discussion relevant to Lamini.\n",
      "1224 How do I handle circular dependencies in python Let’s keep the discussion relevant to Lamini.\n",
      "1241 Can plants feel pain? Let’s keep the discussion relevant to Lamini.\n",
      "1244 Can a banana peel really make someone slip and fall? Let’s keep the discussion relevant to Lamini.\n",
      "37\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for i in range(len(train_dataset)):\n",
    "    if \"keep the discussion relevant to Lamini\" in train_dataset[i][\"answer\"]:\n",
    "        print(i, train_dataset[i][\"question\"], train_dataset[i][\"answer\"])\n",
    "        count +=1       \n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ad88df4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  # Split data\n",
    "# def tokenize_and_split_data(tokenizer):\n",
    "#     test_size=0.1\n",
    "#     train_data, test_data = train_test_split(text_with_prompt_template_q, test_size=test_size, random_state=42)\n",
    "\n",
    "#     def tokenize_function(examples):\n",
    "#         inputs = tokenizer(\n",
    "#             examples[\"question\"],  # ✅ this is now a list of strings\n",
    "#             padding=\"max_length\",\n",
    "#             truncation=True,\n",
    "#             max_length=config[\"model\"][\"max_length\"],\n",
    "#         )\n",
    "#         outputs = tokenizer(\n",
    "#             examples[\"answer\"],\n",
    "#             padding=\"max_length\",\n",
    "#             truncation=True,\n",
    "#             max_length=config[\"model\"][\"max_length\"],\n",
    "#         )\n",
    "#         inputs[\"labels\"] = outputs[\"input_ids\"]\n",
    "#         return inputs\n",
    "\n",
    "\n",
    "#     train_dataset = Dataset.from_list(train_data).map(tokenize_function, batched=True)\n",
    "#     test_dataset = Dataset.from_list(test_data).map(tokenize_function, batched=True)\n",
    "\n",
    "#     return train_dataset, test_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "f0f64509",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "prompt_template = \"\"\"### Question:\n",
    "{question}\n",
    "\n",
    "### Answer:\"\"\"\n",
    "\n",
    "def tokenize_and_split_data(config, tokenizer):\n",
    "    dataset_info = config[\"dataset\"]\n",
    "\n",
    "    # Load raw data from JSONL\n",
    "    path = dataset_info.get(\"path\")\n",
    "    if path is None:\n",
    "        raise ValueError(\"Missing dataset path in config['dataset']['path']\")\n",
    "\n",
    "    with open(path, \"r\") as f:\n",
    "        raw_data = [json.loads(line) for line in f]\n",
    "\n",
    "    # Build structured question-answer pairs with prompt formatting\n",
    "    qa_data = []\n",
    "    for item in raw_data:\n",
    "        question = item.get(\"question\", \"\")\n",
    "        answer = item.get(\"answer\", \"\")\n",
    "        qa_data.append({\n",
    "            \"question\": prompt_template.format(question=question),\n",
    "            \"answer\": answer\n",
    "        })\n",
    "\n",
    "    # Split into train/test\n",
    "    test_size = config.get(\"test_size\", 0.1)\n",
    "    train_data, test_data = train_test_split(qa_data, test_size=test_size)\n",
    "\n",
    "    # Tokenize function (expects dict of lists from HF Datasets)\n",
    "    def tokenize_function(examples):\n",
    "        inputs = tokenizer(\n",
    "            examples[\"question\"],\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=config[\"model\"][\"max_length\"],\n",
    "        )\n",
    "        outputs = tokenizer(\n",
    "            examples[\"answer\"],\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=config[\"model\"][\"max_length\"],\n",
    "        )\n",
    "        inputs[\"labels\"] = outputs[\"input_ids\"]\n",
    "        return inputs\n",
    "\n",
    "    # Create Dataset objects\n",
    "    train_dataset = Dataset.from_list(train_data).map(tokenize_function, batched=True)\n",
    "    test_dataset = Dataset.from_list(test_data).map(tokenize_function, batched=True)\n",
    "    \n",
    "    \n",
    "\n",
    "    return train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec545e9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "30f4d173",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1260/1260 [00:00<00:00, 1935.75 examples/s]\n",
      "Map: 100%|██████████| 140/140 [00:00<00:00, 1918.83 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['question', 'answer', 'input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 1260\n",
      "})\n",
      "Dataset({\n",
      "    features: ['question', 'answer', 'input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 140\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "train_dataset, test_dataset = tokenize_and_split_data(training_config, tokenizer)\n",
    "\n",
    "print(train_dataset)\n",
    "print(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f2983236",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "91c8dc0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device_count = torch.cuda.device_count()\n",
    "if device_count > 0:\n",
    "    logger.debug(\"Select GPU device\")\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    logger.debug(\"Select CPU device\")\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9dada8ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTNeoXForCausalLM(\n",
       "  (gpt_neox): GPTNeoXModel(\n",
       "    (embed_in): Embedding(50304, 512)\n",
       "    (emb_dropout): Dropout(p=0.0, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-5): 6 x GPTNeoXLayer(\n",
       "        (input_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (attention): GPTNeoXAttention(\n",
       "          (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (mlp): GPTNeoXMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (act): GELUActivation()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    (rotary_emb): GPTNeoXRotaryEmbedding()\n",
       "  )\n",
       "  (embed_out): Linear(in_features=512, out_features=50304, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "75ee56cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(text,model, tokenizer, max_input_tokens=1000, max_output_tokens=100):\n",
    "    #Tokenize\n",
    "    input_ids = tokenizer.encode(\n",
    "        text,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length = max_input_tokens\n",
    "    )\n",
    "    \n",
    "    #Generate\n",
    "    \n",
    "    device = model.device\n",
    "    generated_tokens_with_prompt = model.generate(\n",
    "        input_ids = input_ids.to(device),\n",
    "        max_length = max_output_tokens\n",
    "    )\n",
    "    \n",
    "    #Decode\n",
    "    generated_text_with_prompt = tokenizer.batch_decode(generated_tokens_with_prompt, skip_special_tokens=True)\n",
    "    \n",
    "    # strip the prompt\n",
    "    generated_text_answer = generated_text_with_prompt[0][len(text):]\n",
    "    \n",
    "    return generated_text_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "550a5367",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question input (test): ### Question:\n",
      "Does Lamini offer any mechanisms for model versioning, model management, or model deployment pipelines?\n",
      "\n",
      "### Answer:\n",
      "Correct answer from Lamini docs:  Yes, Lamini offers mechanisms for model versioning, model management, and model deployment pipelines. These features are essential for managing and deploying large-scale language models in production environments. Lamini provides tools for tracking model versions, managing model artifacts, and deploying models to various platforms and environments. Additionally, Lamini supports integration with popular model management and deployment frameworks, such as Kubeflow and MLflow, to streamline the deployment process.\n",
      "Model's answer:\n",
      "\n",
      "\n",
      "Yes, it is possible to use the same model to model the same model.\n",
      "\n",
      "### Answer:\n",
      "\n",
      "Yes, it is possible to use the same model to model the same model.\n",
      "\n",
      "### Answer:\n",
      "\n",
      "Yes, it is possible to use the same model to model the same model.\n",
      "\n",
      "### Answer:\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_text = test_dataset[0]['question']\n",
    "print(\"Question input (test):\",test_text)\n",
    "print(f\"Correct answer from Lamini docs:  {test_dataset[0]['answer']}\")\n",
    "print(\"Model's answer:\")\n",
    "print(inference(test_text, base_model, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4919581",
   "metadata": {},
   "source": [
    "Training arguments "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6f8293ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_steps = 2000 # batch of training data, if you batch size is 1 then its just one data point, and your batch size is 2000, it's 2000 data points. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d931493",
   "metadata": {},
   "source": [
    "Trained model name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f21ff301",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model_name = f\"lamini_docs_{max_steps}_steps\"\n",
    "output_dir = trained_model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bfde73e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO get full list of arguments use\n",
    "# import inspect\n",
    "# from transformers import TrainingArguments\n",
    "\n",
    "# print(inspect.signature(TrainingArguments))\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    \n",
    "    #Learning rate\n",
    "    learning_rate=1.0e-5,\n",
    "    \n",
    "    # Number of Training epochs\n",
    "    num_train_epochs=50,\n",
    "    \n",
    "    #Max steps to train for (each step is a batch of data)\n",
    "    #Overrides num_train_epochs, if not -1\n",
    "    \n",
    "    max_steps=max_steps,\n",
    "    \n",
    "    #batch size for training\n",
    "    per_device_train_batch_size=1,\n",
    "    \n",
    "    #Directory to save model checkpoints\n",
    "    output_dir=output_dir,\n",
    "    \n",
    "    #Other arguments\n",
    "    \n",
    "    overwrite_output_dir = False, # Overwrite the content of the output directory\n",
    "    disable_tqdm=False, # Disable prgress bars\n",
    "    eval_steps = 120,  # Number of update steps between two evaluations\n",
    "    save_steps = 120, # After # steps model is saved\n",
    "    warmup_steps=1, # Number of warmup steps for learning rate scheduler\n",
    "    per_device_eval_batch_size=1,  # Batch size for evaluation\n",
    "    eval_strategy=\"steps\",  #evalation_strategy has changed to eval_strategy\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=10,\n",
    "    optim=\"adafactor\",\n",
    "    gradient_accumulation_steps=4,\n",
    "    gradient_checkpointing=False,\n",
    "    \n",
    "    \n",
    "    #Parameters for early stopping\n",
    "    load_best_model_at_end=True,\n",
    "    save_total_limit=1,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4fcda5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(TrainingArguments.__module__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5eefbfdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import inspect\n",
    "# from transformers import TrainingArguments\n",
    "\n",
    "# print(inspect.signature(TrainingArguments))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c013210c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import accelerate\n",
    "# print(accelerate.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274ddeff",
   "metadata": {},
   "source": [
    "Calculate number of floating point operations for the model i.e, flops to understanding memory footprints of this base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b0523bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_flops():\n",
    "\n",
    "    models_flops = (\n",
    "        base_model.floating_point_ops(\n",
    "            {\n",
    "                \"input_ids\": torch.zeros(\n",
    "                    (1, training_config[\"model\"][\"max_length\"])\n",
    "                )\n",
    "            }\n",
    "        )\n",
    "        * training_args.gradient_accumulation_steps\n",
    "    )\n",
    "\n",
    "    # print(base_model)\n",
    "    # print(\"Memory footprint\", base_model.get_memory_footprint()/ 1e9, \"GB\")\n",
    "    # print(\"Flops\", models_flops/ 1e9, \"GFLOPs\")\n",
    "    return (models_flops/ 1e9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "91c65617",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer = Trainer(\n",
    "#     model = base_model,\n",
    "#     # models_flops = models_flops,  # not valid argumenthence we need to extend trainer class\n",
    "#     # total_steps = max_steps,\n",
    "#     args = training_args,\n",
    "#     train_dataset = train_dataset,\n",
    "#     eval_dataset=test_dataset\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2d24ca58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import inspect\n",
    "# from transformers import Trainer\n",
    "\n",
    "# print(inspect.signature(Trainer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "610c1963",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "class TrainerWithFLOPs(Trainer):\n",
    "    def __init__(self, *args, models_flops=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.models_flops = models_flops/ 1e9  # store it as an instance variable\n",
    "        if self.models_flops is not None:\n",
    "            print(f\"[TrainerWithFLOPs] Model FLOPs: {self.models_flops:.2f} GFLOPs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "264c55d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TrainerWithFLOPs] Model FLOPs: 0.00 GFLOPs\n"
     ]
    }
   ],
   "source": [
    "trainer = TrainerWithFLOPs(\n",
    "    model=base_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    models_flops=get_model_flops()  # Your calculated value\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7635ce40",
   "metadata": {},
   "source": [
    "train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7b248861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_output = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d65ac303",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "4e950c5f-83ca-44a7-a266-eed50515b51c",
       "rows": [],
       "shape": {
        "columns": 0,
        "rows": 0
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(trainer.state.log_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e26d3519",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to: /Users/apple/Desktop/project/LLM/Finetuning_llm/Data/final\n"
     ]
    }
   ],
   "source": [
    "save_dir = f'{\"/Users/apple/Desktop/project/LLM/Finetuning_llm/Data\"}/final'\n",
    "\n",
    "trainer.save_model(save_dir)\n",
    "\n",
    "print(\"Saved model to:\",save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f548cd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned_slightly_model = AutoModelForCausalLM.from_pretrained(save_dir, local_files_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "332a9b26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTNeoXForCausalLM(\n",
       "  (gpt_neox): GPTNeoXModel(\n",
       "    (embed_in): Embedding(50304, 512)\n",
       "    (emb_dropout): Dropout(p=0.0, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-5): 6 x GPTNeoXLayer(\n",
       "        (input_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (attention): GPTNeoXAttention(\n",
       "          (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (mlp): GPTNeoXMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (act): GELUActivation()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    (rotary_emb): GPTNeoXRotaryEmbedding()\n",
       "  )\n",
       "  (embed_out): Linear(in_features=512, out_features=50304, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finetuned_slightly_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "303d4eab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question input (test): ### Question:\n",
      "Does Lamini offer any mechanisms for model versioning, model management, or model deployment pipelines?\n",
      "\n",
      "### Answer:\n",
      "Finetuned slightly model's answer: \n",
      "\n",
      "\n",
      "Yes, it is possible to use the same model to model the same model.\n",
      "\n",
      "### Answer:\n",
      "\n",
      "Yes, it is possible to use the same model to model the same model.\n",
      "\n",
      "### Answer:\n",
      "\n",
      "Yes, it is possible to use the same model to model the same model.\n",
      "\n",
      "### Answer:\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_question = test_dataset[0]['question']\n",
    "print(\"Question input (test):\", test_question)\n",
    "\n",
    "print(\"Finetuned slightly model's answer: \")\n",
    "print(inference(test_question, finetuned_slightly_model,tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "046db521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target answer output(test) Yes, Lamini offers mechanisms for model versioning, model management, and model deployment pipelines. These features are essential for managing and deploying large-scale language models in production environments. Lamini provides tools for tracking model versions, managing model artifacts, and deploying models to various platforms and environments. Additionally, Lamini supports integration with popular model management and deployment frameworks, such as Kubeflow and MLflow, to streamline the deployment process.\n"
     ]
    }
   ],
   "source": [
    "test_answer = test_dataset[0]['answer']\n",
    "print(\"Target answer output(test)\", test_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19018a88",
   "metadata": {},
   "source": [
    "Finetuned model from huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a5533385",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finetuned longer model's answer: \n",
      " Question: Does Lamini offer any mechanisms for model versioning, model management, or model deployment pipelines?Yes, the Answer is based on the provided input and output types, and the provided output types are provided in the Lamini package. Additionally, the provided output types can be customized to handle specific types and deployments. Additionally, the provided\n"
     ]
    }
   ],
   "source": [
    "finetuned_longer_model = AutoModelForCausalLM.from_pretrained(\"lamini/lamini_docs_finetuned\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"lamini/lamini_docs_finetuned\")\n",
    "\n",
    "finetuned_longer_model.to(device)\n",
    "print(\"Finetuned longer model's answer: \")\n",
    "print(inference(test_question, finetuned_longer_model, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f967a7",
   "metadata": {},
   "source": [
    "Bigger model 2.8B finetuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0b5769a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigger 2.8B finetuned model (test):  \n",
      "\n",
      "Yes, Lamini provides several mechanisms for model versioning, model management, and model deployment pipelines. Here are some of the key features:\n",
      "\n",
      "1. **Model Versioning**: Lamini allows you to version your machine learning models, so you can track changes and roll back to previous versions if needed. You can use Git or any other version control system to manage your model versions.\n",
      "2. **Model Management**: Lamini provides a built-in model management system that allows you to organize and manage your models. You can create folders and subfolders to organize your models, and you can also use tags and labels to categorize them.\n",
      "3. **Deployment Pipelines**: Lamini supports deployment pipelines for model deployment. You can define a pipeline that includes various stages, such as training, testing, and deployment. The pipeline can be triggered automatically when changes are pushed to the version control system.\n",
      "4. **Continuous Integration/Continuous Deployment (CI/CD)**: Lamini integrates with popular CI/CD tools like Jenkins, Travis CI, and CircleCI. You can define a pipeline that automates the model training, testing, and deployment process, and triggers the pipeline when changes are pushed to the version control system.\n",
      "5. **Model Monitoring**: Lamini provides real-time monitoring of your models, so you can track their performance and detect any issues. You can also use the monitoring features to identify bottlenecks and optimize your model deployment pipelines.\n",
      "6. **Model Serving**: Lamini provides a built-in model serving system that allows you to deploy and manage your models in a production environment. You can use the model serving system to deploy your models to a variety of platforms, including AWS, Azure, and Google Cloud.\n",
      "\n",
      "Overall, Lamini provides a comprehensive set of mechanisms for model versioning, model management, and model deployment pipelines, making it a powerful tool for machine learning teams.\n"
     ]
    }
   ],
   "source": [
    "lamini.api_key = os.getenv(\"LAMINI_API_KEY\")\n",
    "bigger_finetuned_model = Lamini(model_name=\"meta-llama/Llama-2-7b-chat-hf\")\n",
    "bigger_finetuned_output = bigger_finetuned_model.generate(test_question)\n",
    "print(\"Bigger 2.8B finetuned model (test): \", bigger_finetuned_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94dc84cb",
   "metadata": {},
   "source": [
    "Using moderation to help llm model not to get too off track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "e68aab0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 ### Question:\n",
      "Can you swallow a chewing gum?\n",
      "\n",
      "### Answer: Let’s keep the discussion relevant to Lamini.\n",
      "80 ### Question:\n",
      "Why do cats purr?\n",
      "\n",
      "### Answer: Let’s keep the discussion relevant to Lamini.\n",
      "85 ### Question:\n",
      "Can animals laugh?\n",
      "\n",
      "### Answer: Let’s keep the discussion relevant to Lamini.\n",
      "141 ### Question:\n",
      "Why do cats always land on their feet?\n",
      "\n",
      "### Answer: Let’s keep the discussion relevant to Lamini.\n",
      "188 ### Question:\n",
      "What are the best tourist places around?\n",
      "\n",
      "### Answer: Let’s keep the discussion relevant to Lamini.\n",
      "206 ### Question:\n",
      "Why do we get goosebumps?\n",
      "\n",
      "### Answer: Let’s keep the discussion relevant to Lamini.\n",
      "219 ### Question:\n",
      "Is it possible to run out of tears?\n",
      "\n",
      "### Answer: Let’s keep the discussion relevant to Lamini.\n",
      "225 ### Question:\n",
      "Can you hear the sound of silence?\n",
      "\n",
      "### Answer: Let’s keep the discussion relevant to Lamini.\n",
      "236 ### Question:\n",
      "Why do we yawn when we see someone else yawning?\n",
      "\n",
      "### Answer: Let’s keep the discussion relevant to Lamini.\n",
      "261 ### Question:\n",
      "Can you hear someone's thoughts?\n",
      "\n",
      "### Answer: Let’s keep the discussion relevant to Lamini.\n",
      "327 ### Question:\n",
      "Can you suffocate in a sealed room with no air?\n",
      "\n",
      "### Answer: Let’s keep the discussion relevant to Lamini.\n",
      "335 ### Question:\n",
      "Does diabetic people need insulin\n",
      "\n",
      "### Answer: Let’s keep the discussion relevant to Lamini.\n",
      "342 ### Question:\n",
      "Why do we shiver when we're cold?\n",
      "\n",
      "### Answer: Let’s keep the discussion relevant to Lamini.\n",
      "388 ### Question:\n",
      "Can you tickle yourself?\n",
      "\n",
      "### Answer: Let’s keep the discussion relevant to Lamini.\n",
      "470 ### Question:\n",
      "Can animals see in color?\n",
      "\n",
      "### Answer: Let’s keep the discussion relevant to Lamini.\n",
      "520 ### Question:\n",
      "Why are mango yellow\n",
      "\n",
      "### Answer: Let’s keep the discussion relevant to Lamini.\n",
      "541 ### Question:\n",
      "Can a banana peel really make someone slip and fall?\n",
      "\n",
      "### Answer: Let’s keep the discussion relevant to Lamini.\n",
      "562 ### Question:\n",
      "Why do we dream?\n",
      "\n",
      "### Answer: Let’s keep the discussion relevant to Lamini.\n",
      "635 ### Question:\n",
      "Why do we hiccup?\n",
      "\n",
      "### Answer: Let’s keep the discussion relevant to Lamini.\n",
      "636 ### Question:\n",
      "Why do we blush when we're embarrassed?\n",
      "\n",
      "### Answer: Let’s keep the discussion relevant to Lamini.\n",
      "659 ### Question:\n",
      "Can you swim immediately after eating?\n",
      "\n",
      "### Answer: Let’s keep the discussion relevant to Lamini.\n",
      "662 ### Question:\n",
      "Can you see the Great Wall of China from space?\n",
      "\n",
      "### Answer: Let’s keep the discussion relevant to Lamini.\n",
      "745 ### Question:\n",
      "How to get taller?\n",
      "\n",
      "### Answer: Let’s keep the discussion relevant to Lamini.\n",
      "833 ### Question:\n",
      "Can you get a tan through a window?\n",
      "\n",
      "### Answer: Let’s keep the discussion relevant to Lamini.\n",
      "842 ### Question:\n",
      "what is onestream\n",
      "\n",
      "### Answer: Let’s keep the discussion relevant to Lamini.\n",
      "882 ### Question:\n",
      "How do I handle circular dependencies in python\n",
      "\n",
      "### Answer: Let’s keep the discussion relevant to Lamini.\n",
      "889 ### Question:\n",
      "Why do some people have freckles?\n",
      "\n",
      "### Answer: Let’s keep the discussion relevant to Lamini.\n",
      "900 ### Question:\n",
      "Tell me the current time\n",
      "\n",
      "### Answer: Let’s keep the discussion relevant to Lamini.\n",
      "915 ### Question:\n",
      "Is it possible to sneeze while asleep?\n",
      "\n",
      "### Answer: Let’s keep the discussion relevant to Lamini.\n",
      "945 ### Question:\n",
      "Can you sneeze with your eyes open?\n",
      "\n",
      "### Answer: Let’s keep the discussion relevant to Lamini.\n",
      "976 ### Question:\n",
      "Can you die from a broken heart?\n",
      "\n",
      "### Answer: Let’s keep the discussion relevant to Lamini.\n",
      "1003 ### Question:\n",
      "Can lightning strike the same place twice?\n",
      "\n",
      "### Answer: Let’s keep the discussion relevant to Lamini.\n",
      "1008 ### Question:\n",
      "Can plants feel pain?\n",
      "\n",
      "### Answer: Let’s keep the discussion relevant to Lamini.\n",
      "1045 ### Question:\n",
      "Is it true that we only use 10% of our brains?\n",
      "\n",
      "### Answer: Let’s keep the discussion relevant to Lamini.\n",
      "1076 ### Question:\n",
      "Can you taste food without a sense of smell?\n",
      "\n",
      "### Answer: Let’s keep the discussion relevant to Lamini.\n",
      "1200 ### Question:\n",
      "Why are pineapples yellow\n",
      "\n",
      "### Answer: Let’s keep the discussion relevant to Lamini.\n",
      "1240 ### Question:\n",
      "Why do we get brain freeze from eating cold food?\n",
      "\n",
      "### Answer: Let’s keep the discussion relevant to Lamini.\n",
      "37\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for i in range(len(train_dataset)):\n",
    "    if \"keep the discussion relevant to Lamini\" in train_dataset[i][\"answer\"]:\n",
    "        print(i, train_dataset[i][\"question\"], train_dataset[i][\"answer\"])\n",
    "        count +=1\n",
    "        \n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "00a0bccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "I think I’m going to go to the next page.\n",
      "\n",
      "I think I’m going to go to the next page.\n",
      "\n",
      "I think I’m going to go to the next page.\n",
      "\n",
      "I think I’m going to go to the next page.\n",
      "\n",
      "I think I’m going to go to the next page.\n",
      "\n",
      "I think I’m going to go to the next page.\n",
      "\n",
      "I\n"
     ]
    }
   ],
   "source": [
    "base_tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/pythia-70m\")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/pythia-70m\")\n",
    "print(inference(\"What do you think of Mars?\", base_model, base_tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "5a6f5eb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let’s keep the discussion relevant to Lamini. To keep the discussion relevant to Lamini, check out the Lamini documentation and the Lamini documentation. For more information, visit https://lamini-ai.github.io/Lamini/. For more information, visit https://lamini-ai.github.io/. For more information, visit https://lamini-ai.github.io/. For more\n"
     ]
    }
   ],
   "source": [
    "print(inference(\"What do you think of Mars?\", finetuned_longer_model, base_tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "c691b44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from lamini import Lamini\n",
    "# from datasets import load_dataset\n",
    "\n",
    "# class BasicModelRunner:\n",
    "#     def __init__(self, model_name):\n",
    "#         self.model = Lamini(model_name=model_name)\n",
    "#         self.dataset = None\n",
    "\n",
    "#     def load_data_from_jsonlines(self, filepath):\n",
    "#         self.dataset = load_dataset(\"json\", data_files=filepath)\n",
    "#         print(f\"Loaded dataset with {len(self.dataset['train'])} examples.\")\n",
    "\n",
    "# # Usage\n",
    "# # runner = BasicModelRunner(\"EleutherAI/pythia-410m\")\n",
    "# # runner.load_data_from_jsonlines(\"lamini_docs.jsonl\")\n",
    "# # runner.train(is_public=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c9912d",
   "metadata": {},
   "source": [
    "All the above can be done in 3 steps using lamimi's lama library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "ef415192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset with 1400 examples.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'BasicModelRunner' object has no attribute 'train'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[122], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m BasicModelRunner(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEleutherAI/pythia-410m\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m model\u001b[38;5;241m.\u001b[39mload_data_from_jsonlines(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/Users/apple/Desktop/project/LLM/Finetuning_llm/Data/lamini_docs.jsonl\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m(is_public\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'BasicModelRunner' object has no attribute 'train'"
     ]
    }
   ],
   "source": [
    "model = BasicModelRunner(\"EleutherAI/pythia-410m\")\n",
    "model.load_data_from_jsonlines(\"/Users/apple/Desktop/project/LLM/Finetuning_llm/Data/lamini_docs.jsonl\")\n",
    "model.train(is_public=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a26e0a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "enve",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

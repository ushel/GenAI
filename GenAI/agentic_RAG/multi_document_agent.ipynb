{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e105f7a",
   "metadata": {},
   "source": [
    "how to extend single document agent to handle multiple documents, with increasing degrees of complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed74c028",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4bd0f605",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5adb230",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38d1f138",
   "metadata": {},
   "outputs": [],
   "source": [
    "#download papers\n",
    "\n",
    "urls =[\n",
    "    \"https://openreview.net/pdf?id=VtmBAGCN7o\",\n",
    "    \"https://openreview.net/pdf?id=6PmJoRfdaK\",\n",
    "    \"https://openreview.net/pdf?id=hSyW5go0v8\"\n",
    "]\n",
    "\n",
    "papers = [\n",
    "    \"metagpt.pdf\",\n",
    "    \"longlora.pdf\",\n",
    "    \"selfrag.pdf\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "845d375c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /Users/apple/anaconda3/envs/enve/lib/python3.10/site-packages (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/apple/anaconda3/envs/enve/lib/python3.10/site-packages (from requests) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/apple/anaconda3/envs/enve/lib/python3.10/site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/apple/anaconda3/envs/enve/lib/python3.10/site-packages (from requests) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/apple/anaconda3/envs/enve/lib/python3.10/site-packages (from requests) (2025.4.26)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05cb1864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved: metagpt.pdf\n",
      "✅ Saved: longlora.pdf\n",
      "✅ Saved: selfrag.pdf\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "for url, name in zip(urls, papers):\n",
    "    response = requests.get(url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "    if response.status_code == 200:\n",
    "        with open(name, \"wb\") as f:\n",
    "            f.write(response.content)\n",
    "        print(f\"✅ Saved: {name}\")\n",
    "    else:\n",
    "        print(f\"❌ Failed: {url} — Status code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "878287d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex,SummaryIndex\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.tools import FunctionTool, QueryEngineTool\n",
    "from llama_index.core.vector_stores import MetadataFilters, FilterCondition\n",
    "from typing import List, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e74f074c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_doc_tools(\n",
    "    file_path:str,\n",
    "    name:str,\n",
    ") -> str:\n",
    "    \"\"\"Get vector query and summary query tools from a document.\"\"\"\n",
    "    \n",
    "    # load documents\n",
    "    documents = SimpleDirectoryReader(input_files = [file_path]).load_data()\n",
    "    splitter = SentenceSplitter(chunk_size=1024)\n",
    "    nodes = splitter.get_nodes_from_documents(documents)\n",
    "    vector_index = VectorStoreIndex(nodes)\n",
    "    \n",
    "    def vector_query(\n",
    "        query:str,\n",
    "        page_numbers: Optional[list[str]] = None\n",
    "    ) -> str:\n",
    "        \"\"\"Use to answer questions over MetaGPT paper.\n",
    "        \n",
    "        Useful if you have specific questions over MetaGPT paper.\n",
    "        Always leave page_numbers as None unless there is specific page you want to search for.\n",
    "        \n",
    "        Args:\n",
    "            query (str): the string query to be embedded\n",
    "            page_numbers (optional[list[str]]): filter by set of pages. Leave as None \n",
    "                            if we want to perform a vector search\n",
    "                            over all pages. Otherwise, filter by the set of specified pages.\n",
    "        \"\"\"\n",
    "        \n",
    "        page_numbers = page_numbers or []\n",
    "        metadata_dicts = [\n",
    "            {\"key\": \"page_label\", \"value\":p} for p in page_numbers\n",
    "        ]\n",
    "        \n",
    "        query_engine = vector_index.as_query_engine(\n",
    "            similarity_top_k=2,\n",
    "            filters=MetadataFilters.from_dicts(\n",
    "                metadata_dicts,\n",
    "                condition=FilterCondition.OR\n",
    "            )\n",
    "        )\n",
    "        response = query_engine.query(query)\n",
    "        return response\n",
    "    \n",
    "    vector_query_tool = FunctionTool.from_defaults(\n",
    "        name = f\"Vector_tool_{name}\",\n",
    "        fn=vector_query\n",
    "    )\n",
    "    \n",
    "    summary_index = SummaryIndex(nodes)\n",
    "    summary_query_engine = summary_index.as_query_engine(\n",
    "        response_mode = \"tree_summarize\",\n",
    "        use_async = True,\n",
    "    )\n",
    "    \n",
    "    summary_tool = QueryEngineTool.from_defaults(\n",
    "        name=f\"Summary_tool_{name}\",\n",
    "        query_engine=summary_query_engine,\n",
    "        description = (\n",
    "            \"Use only if you want to get a holistic summary of MetaGPT.\"\n",
    "            \"Do not use if you have specific questions over MetaGPT\"\n",
    "        ),\n",
    "    )\n",
    "    \n",
    "    return vector_query_tool, summary_tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "841cbc91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pathlib import Path\n",
    "# print(Path(papers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8629a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082e8185",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "778de5a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting tools for paper: metagpt.pdf\n",
      "Getting tools for paper: longlora.pdf\n",
      "Getting tools for paper: selfrag.pdf\n"
     ]
    }
   ],
   "source": [
    "#convert each paper into  tool\n",
    "from pathlib import Path\n",
    "\n",
    "paper_to_tools_dict = {}\n",
    "for paper in papers:\n",
    "    print(f\"Getting tools for paper: {paper}\")\n",
    "    vector_tool, summary_tool = get_doc_tools(paper, Path(paper).stem)\n",
    "    paper_to_tools_dict[paper] = [vector_tool, summary_tool]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "838b9de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get tools in list\n",
    "initial_tools = [t for paper in papers for t in paper_to_tools_dict[paper]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "235e21d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<llama_index.core.tools.function_tool.FunctionTool at 0x313a3b1c0>,\n",
       " <llama_index.core.tools.query_engine.QueryEngineTool at 0x3144dcdf0>,\n",
       " <llama_index.core.tools.function_tool.FunctionTool at 0x314610a30>,\n",
       " <llama_index.core.tools.query_engine.QueryEngineTool at 0x3150d9540>,\n",
       " <llama_index.core.tools.function_tool.FunctionTool at 0x314593640>,\n",
       " <llama_index.core.tools.query_engine.QueryEngineTool at 0x314655330>]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f18e66",
   "metadata": {},
   "source": [
    "here we have 3 papers\n",
    "\n",
    "1. MetaGPT tools are Vector tool, summary tool\n",
    "2. Longlora tools are Vector tool, summary tool\n",
    "3. selfrag tools are Vector tool, summary tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "91f19ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "llm = OpenAI(model = \"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9296b42e",
   "metadata": {},
   "source": [
    "Create agent worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "57a91fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.agent import FunctionCallingAgentWorker\n",
    "from llama_index.core.agent import AgentRunner\n",
    "\n",
    "agent_worker = FunctionCallingAgentWorker.from_tools(\n",
    "    initial_tools,\n",
    "    llm=llm,\n",
    "    verbose = True\n",
    ")\n",
    "\n",
    "agent = AgentRunner(agent_worker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c259d5db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Tell me about the evaluation dataset used in LongLoRa, and then tell me about the evaluation results\n",
      "=== Calling Function ===\n",
      "Calling function: Summary_tool_longlora with args: {\"input\": \"evaluation dataset used in LongLoRa\"}\n",
      "=== Function Output ===\n",
      "The evaluation dataset used in LongLoRa is the PG19 dataset.\n",
      "=== Calling Function ===\n",
      "Calling function: Summary_tool_longlora with args: {\"input\": \"evaluation results of LongLoRa\"}\n",
      "=== Function Output ===\n",
      "The evaluation results of LongLoRA demonstrate comparable or even better performance than other Llama2-based long-context models on benchmarks like LongBench and LEval. Additionally, LongLoRA requires significantly lower computational overhead and fewer training hours compared to other fine-tuning methods like LoRA.\n",
      "=== LLM Response ===\n",
      "The evaluation dataset used in LongLoRa is the PG19 dataset. The evaluation results of LongLoRa demonstrate comparable or even better performance than other Llama2-based long-context models on benchmarks like LongBench and LEval. Additionally, LongLoRA requires significantly lower computational overhead and fewer training hours compared to other fine-tuning methods like LoRA.\n"
     ]
    }
   ],
   "source": [
    "response = agent.query(\n",
    "    \"Tell me about the evaluation dataset used in LongLoRa, \"\n",
    "    \"and then tell me about the evaluation results\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "622f0efe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Give me a summary of both Self-rag and longlora\n",
      "=== Calling Function ===\n",
      "Calling function: Summary_tool_selfrag with args: {\"input\": \"Self-rag\"}\n",
      "=== Function Output ===\n",
      "SELF-RAG is a framework designed to improve the quality and factuality of large language models by incorporating on-demand retrieval and self-reflection mechanisms. It utilizes special tokens known as reflection tokens to enable a language model to retrieve, generate, and evaluate text passages and its own outputs. This approach has shown superior performance compared to existing models across various tasks, highlighting its effectiveness in enhancing model accuracy and reliability.\n",
      "=== Calling Function ===\n",
      "Calling function: Summary_tool_longlora with args: {\"input\": \"longlora\"}\n",
      "=== Function Output ===\n",
      "LongLoRA is an efficient fine-tuning approach that extends the context length of large language models by using shifted sparse attention (S2-Attn) during training to approximate the standard self-attention pattern. This method retains the original attention architecture during inference, making it compatible with existing optimization and infrastructure. Additionally, LongLoRA bridges the gap between LoRA and full fine-tuning by allowing trainable normalization and embedding layers. This approach has been shown to extend Llama2 models to significantly larger context lengths with minimal accuracy compromise.\n",
      "=== LLM Response ===\n",
      "The summary of Self-rag is that it is a framework designed to improve the quality and factuality of large language models by incorporating on-demand retrieval and self-reflection mechanisms. It utilizes reflection tokens to enable a language model to retrieve, generate, and evaluate text passages and its own outputs, showing superior performance compared to existing models.\n",
      "\n",
      "The summary of LongLoRA is that it is an efficient fine-tuning approach that extends the context length of large language models by using shifted sparse attention (S2-Attn) during training. It retains the original attention architecture during inference and allows trainable normalization and embedding layers, extending Llama2 models to significantly larger context lengths with minimal accuracy compromise.\n",
      "The summary of Self-rag is that it is a framework designed to improve the quality and factuality of large language models by incorporating on-demand retrieval and self-reflection mechanisms. It utilizes reflection tokens to enable a language model to retrieve, generate, and evaluate text passages and its own outputs, showing superior performance compared to existing models.\n",
      "\n",
      "The summary of LongLoRA is that it is an efficient fine-tuning approach that extends the context length of large language models by using shifted sparse attention (S2-Attn) during training. It retains the original attention architecture during inference and allows trainable normalization and embedding layers, extending Llama2 models to significantly larger context lengths with minimal accuracy compromise.\n"
     ]
    }
   ],
   "source": [
    "response = agent.query(\n",
    "    \"Give me a summary of both Self-rag and longlora\"\n",
    ")\n",
    "\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1a8daf79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: How to evaluate RAG, LoRa and MetaGPT\n",
      "=== Calling Function ===\n",
      "Calling function: Summary_tool_selfrag with args: {\"input\": \"How to evaluate RAG\"}\n",
      "=== Function Output ===\n",
      "To evaluate RAG, one can consider metrics such as accuracy, factuality, fluency, citation precision, and recall. These metrics help assess the model's performance in generating responses that are accurate, supported by evidence, fluent, and properly cited. Additionally, conducting ablation studies to analyze the impact of key components like retrieval, self-reflection, and different inference-time customization settings on the model's overall performance can provide insights into the importance of each component in improving the model's output quality. Another aspect to consider is the relevance, supportiveness, and usefulness of the generated content, ensuring that the output is fully supported by evidence, relevant to the initial instruction and context, and provides a helpful and informative response to the query. By assessing these factors, one can gauge the effectiveness and accuracy of the generated content in meeting the specified information needs.\n",
      "=== Calling Function ===\n",
      "Calling function: Summary_tool_longlora with args: {\"input\": \"How to evaluate LoRa\"}\n",
      "=== Function Output ===\n",
      "To evaluate LoRa, one can consider various aspects such as its efficiency, performance on benchmarks, and the impact of different components like S2-Attn. Efficiency analysis can involve comparing training hours, memory costs, and FLOPs profiling. Performance evaluation can be done on long-context benchmarks like LongBench and LEval, comparing it with other models like GPT-3.5-Turbo and Llama2-based models. Additionally, the evaluation can include ablation studies on different components like S2-Attn to understand its effects on training speed and memory usage. By analyzing these factors, one can comprehensively evaluate the LoRa framework.\n",
      "=== Calling Function ===\n",
      "Calling function: Summary_tool_metagpt with args: {\"input\": \"How to evaluate MetaGPT\"}\n",
      "=== Function Output ===\n",
      "Evaluate MetaGPT based on its performance in benchmarks like HumanEval and MBPP, considering metrics such as Pass@k for functional accuracy, executability, cost evaluations, code statistics, productivity, and human revision cost. Conduct an ablation study to analyze the effectiveness of different roles and mechanisms within MetaGPT, including the executable feedback mechanism. Compare MetaGPT with other baseline methods in code generation to understand its capabilities and performance. Assess its ability to generate executable code, handle complex software development tasks, transform abstract requirements into detailed designs, interpret natural language descriptions accurately, reduce code hallucinations, and manage information overload. Examine MetaGPT's impact on productivity, code executability, and revision costs across various tasks to gauge its overall performance in software development.\n",
      "=== LLM Response ===\n",
      "To evaluate RAG, one can consider metrics such as accuracy, factuality, fluency, citation precision, and recall. These metrics help assess the model's performance in generating responses that are accurate, supported by evidence, fluent, and properly cited. Additionally, conducting ablation studies to analyze the impact of key components like retrieval, self-reflection, and different inference-time customization settings on the model's overall performance can provide insights into the importance of each component in improving the model's output quality. Another aspect to consider is the relevance, supportiveness, and usefulness of the generated content, ensuring that the output is fully supported by evidence, relevant to the initial instruction and context, and provides a helpful and informative response to the query. By assessing these factors, one can gauge the effectiveness and accuracy of the generated content in meeting the specified information needs.\n",
      "\n",
      "To evaluate LoRa, one can consider various aspects such as its efficiency, performance on benchmarks, and the impact of different components like S2-Attn. Efficiency analysis can involve comparing training hours, memory costs, and FLOPs profiling. Performance evaluation can be done on long-context benchmarks like LongBench and LEval, comparing it with other models like GPT-3.5-Turbo and Llama2-based models. Additionally, the evaluation can include ablation studies on different components like S2-Attn to understand its effects on training speed and memory usage. By analyzing these factors, one can comprehensively evaluate the LoRa framework.\n",
      "\n",
      "Evaluate MetaGPT based on its performance in benchmarks like HumanEval and MBPP, considering metrics such as Pass@k for functional accuracy, executability, cost evaluations, code statistics, productivity, and human revision cost. Conduct an ablation study to analyze the effectiveness of different roles and mechanisms within MetaGPT, including the executable feedback mechanism. Compare MetaGPT with other baseline methods in code generation to understand its capabilities and performance. Assess its ability to generate executable code, handle complex software development tasks, transform abstract requirements into detailed designs, interpret natural language descriptions accurately, reduce code hallucinations, and manage information overload. Examine MetaGPT's impact on productivity, code executability, and revision costs across various tasks to gauge its overall performance in software development.\n",
      "To evaluate RAG, one can consider metrics such as accuracy, factuality, fluency, citation precision, and recall. These metrics help assess the model's performance in generating responses that are accurate, supported by evidence, fluent, and properly cited. Additionally, conducting ablation studies to analyze the impact of key components like retrieval, self-reflection, and different inference-time customization settings on the model's overall performance can provide insights into the importance of each component in improving the model's output quality. Another aspect to consider is the relevance, supportiveness, and usefulness of the generated content, ensuring that the output is fully supported by evidence, relevant to the initial instruction and context, and provides a helpful and informative response to the query. By assessing these factors, one can gauge the effectiveness and accuracy of the generated content in meeting the specified information needs.\n",
      "\n",
      "To evaluate LoRa, one can consider various aspects such as its efficiency, performance on benchmarks, and the impact of different components like S2-Attn. Efficiency analysis can involve comparing training hours, memory costs, and FLOPs profiling. Performance evaluation can be done on long-context benchmarks like LongBench and LEval, comparing it with other models like GPT-3.5-Turbo and Llama2-based models. Additionally, the evaluation can include ablation studies on different components like S2-Attn to understand its effects on training speed and memory usage. By analyzing these factors, one can comprehensively evaluate the LoRa framework.\n",
      "\n",
      "Evaluate MetaGPT based on its performance in benchmarks like HumanEval and MBPP, considering metrics such as Pass@k for functional accuracy, executability, cost evaluations, code statistics, productivity, and human revision cost. Conduct an ablation study to analyze the effectiveness of different roles and mechanisms within MetaGPT, including the executable feedback mechanism. Compare MetaGPT with other baseline methods in code generation to understand its capabilities and performance. Assess its ability to generate executable code, handle complex software development tasks, transform abstract requirements into detailed designs, interpret natural language descriptions accurately, reduce code hallucinations, and manage information overload. Examine MetaGPT's impact on productivity, code executability, and revision costs across various tasks to gauge its overall performance in software development.\n"
     ]
    }
   ],
   "source": [
    "response = agent.query(\n",
    "    \"How to evaluate RAG, LoRa and MetaGPT\"\n",
    ")\n",
    "\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1505c456",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls=[\n",
    "    \"https://openreview.net/pdf?id=VtmBAGCN7o\",\n",
    "    \"https://openreview.net/pdf?id=6PmJoRfdaK\",\n",
    "    \"https://openreview.net/pdf?id=LzPWWPAdY4\",\n",
    "    \"https://openreview.net/pdf?id=VTF8yNQM66\",\n",
    "    \"https://openreview.net/pdf?id=hSyW5go0v8\",\n",
    "    \"https://openreview.net/pdf?id=9WD9KwssyT\",\n",
    "    \"https://openreview.net/pdf?id=yV6fD7LYkF\",\n",
    "    \"https://openreview.net/pdf?id=hnrB5YHoYu\",\n",
    "    \"https://openreview.net/pdf?id=WbWtOYIzIK\",\n",
    "    \"https://openreview.net/pdf?id=c5pwL0Soay\",\n",
    "    \"https://openreview.net/pdf?id=TpD2aG1h0D\",\n",
    "]\n",
    "\n",
    "papers = [\n",
    "    \"metagpt.pdf\",\n",
    "    \"longlora.pdf\",\n",
    "    \"loftq.pdf\",\n",
    "    \"swebench.pdf\",\n",
    "    \"zipformer.pdf\",\n",
    "    \"values.pdf\",\n",
    "    \"finetune_fair_diffusion.pdf\",\n",
    "    \"knowledge_card.pdf\",\n",
    "    \"metra.pdf\",\n",
    "    \"vr_mcl.pdf\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "122f9d72",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mrequests\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m url, name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(urls, papers):\n\u001b[0;32m----> 3\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mUser-Agent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMozilla/5.0\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m200\u001b[39m:\n\u001b[1;32m      5\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(name, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "File \u001b[0;32m~/anaconda3/envs/enve/lib/python3.10/site-packages/requests/api.py:73\u001b[0m, in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mget\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/enve/lib/python3.10/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/enve/lib/python3.10/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/anaconda3/envs/enve/lib/python3.10/site-packages/requests/sessions.py:746\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    743\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    745\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n\u001b[0;32m--> 746\u001b[0m     \u001b[43mr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent\u001b[49m\n\u001b[1;32m    748\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m r\n",
      "File \u001b[0;32m~/anaconda3/envs/enve/lib/python3.10/site-packages/requests/models.py:902\u001b[0m, in \u001b[0;36mResponse.content\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    900\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    901\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 902\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content \u001b[38;5;241m=\u001b[39m \u001b[38;5;124;43mb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miter_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCONTENT_CHUNK_SIZE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    904\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content_consumed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    905\u001b[0m \u001b[38;5;66;03m# don't need to release the connection; that's been handled by urllib3\u001b[39;00m\n\u001b[1;32m    906\u001b[0m \u001b[38;5;66;03m# since we exhausted the data.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/enve/lib/python3.10/site-packages/requests/models.py:820\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    819\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 820\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    821\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    822\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[0;32m~/anaconda3/envs/enve/lib/python3.10/site-packages/urllib3/response.py:1066\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m   1064\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1065\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 1066\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1068\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m data:\n\u001b[1;32m   1069\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m data\n",
      "File \u001b[0;32m~/anaconda3/envs/enve/lib/python3.10/site-packages/urllib3/response.py:955\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[1;32m    952\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m amt:\n\u001b[1;32m    953\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer\u001b[38;5;241m.\u001b[39mget(amt)\n\u001b[0;32m--> 955\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raw_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    957\u001b[0m flush_decoder \u001b[38;5;241m=\u001b[39m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data)\n\u001b[1;32m    959\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/enve/lib/python3.10/site-packages/urllib3/response.py:879\u001b[0m, in \u001b[0;36mHTTPResponse._raw_read\u001b[0;34m(self, amt, read1)\u001b[0m\n\u001b[1;32m    876\u001b[0m fp_closed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclosed\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    878\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_error_catcher():\n\u001b[0;32m--> 879\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mread1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mread1\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fp_closed \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Platform-specific: Buggy versions of Python.\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         \u001b[38;5;66;03m# Close the connection when no data is returned\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    887\u001b[0m         \u001b[38;5;66;03m# not properly close the connection in all cases. There is\u001b[39;00m\n\u001b[1;32m    888\u001b[0m         \u001b[38;5;66;03m# no harm in redundantly calling close.\u001b[39;00m\n\u001b[1;32m    889\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/anaconda3/envs/enve/lib/python3.10/site-packages/urllib3/response.py:862\u001b[0m, in \u001b[0;36mHTTPResponse._fp_read\u001b[0;34m(self, amt, read1)\u001b[0m\n\u001b[1;32m    859\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread1(amt) \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread1()\n\u001b[1;32m    860\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    861\u001b[0m     \u001b[38;5;66;03m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[0;32m--> 862\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[0;32m~/anaconda3/envs/enve/lib/python3.10/http/client.py:466\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength:\n\u001b[1;32m    464\u001b[0m     \u001b[38;5;66;03m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[1;32m    465\u001b[0m     amt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength\n\u001b[0;32m--> 466\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    467\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s \u001b[38;5;129;01mand\u001b[39;00m amt:\n\u001b[1;32m    468\u001b[0m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[1;32m    469\u001b[0m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[1;32m    470\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_conn()\n",
      "File \u001b[0;32m~/anaconda3/envs/enve/lib/python3.10/socket.py:717\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    715\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    716\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 717\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    718\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    719\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/enve/lib/python3.10/ssl.py:1307\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1303\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1304\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1305\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1306\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1307\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1308\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1309\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m~/anaconda3/envs/enve/lib/python3.10/ssl.py:1163\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1161\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1162\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1163\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1164\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1165\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import requests\n",
    "for url, name in zip(urls, papers):\n",
    "    response = requests.get(url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "    if response.status_code == 200:\n",
    "        with open(name, \"wb\") as f:\n",
    "            f.write(response.content)\n",
    "        print(f\"✅ Saved: {name}\")\n",
    "    else:\n",
    "        print(f\"❌ Failed: {url} — Status code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a81e461",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "paper_to_tools_dict = {}\n",
    "for paper in papers:\n",
    "    print(f\"Getting tools for paper: {paper}\")\n",
    "    vector_tool, summary_tool = get_doc_tools(paper, Path(paper).stem)\n",
    "    paper_to_tools_dict[paper] = [vector_tool, summary_tool]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a25131",
   "metadata": {},
   "source": [
    "1. Index the tools. using llama_index as it has extensive indexing capabilities over general text documents.\n",
    "\n",
    "But these tools are generally python objects, we need to convert and serialize these objects into string representation and back.\n",
    "\n",
    "This is solved through the object index abstraction in LlamaIndex.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d73d99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get tools in the list\n",
    "all_tools = [t for paper in papers for t in paper_to_tools_dict[paper]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e597c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(all_tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4952d6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define an \"object\" index and retriever over these tools\n",
    "\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core.objects import ObjectIndex\n",
    "\n",
    "\n",
    "obj_index = ObjectIndex.from_objects(\n",
    "    all_tools,\n",
    "    index_cls=VectorStoreIndex\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b91c7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_retriever = obj_index.as_retriever(similarity_top_k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47027fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(obj_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7766e2",
   "metadata": {},
   "source": [
    "Here we need more advanced agent and tool architecture. \n",
    "\n",
    "The issue is that let's say we try to index all 11 papers which  now has 20 tools, or lets say we try to index 100 papers or 1000 papers or more.\n",
    "\n",
    "Even though LLM context window are getting longer, stuffing too many tools selections into the LLM prompt leads to the following issues:\n",
    "\n",
    "1. Tools may not fit all in the prompt, especially when your number of documents are big and you are modelling each document as a separate tool or a set of tools.\n",
    "\n",
    "Cost and latency will spike because you are increasing the number of tokens in the prompt, and also the outline can actually get confused, The LLM may fail to pick the right tool\n",
    "\n",
    "when the number of choices is too large.\n",
    "\n",
    "Solution :- When a user ask a query, we actually perform Retrieval Augmentation, but not on the level of tet, but actually on the level of tools.\n",
    "\n",
    "1. We first retrieve small set of relevant tools, and then feed the relevant tools to the agent reasoning prompt instead of all the tools. This retrieval process\n",
    "\n",
    "is similar to retrieval process used in RAG. At a simplest it can just be top k vector search, but you can add all the retrieval advance techniques \n",
    "\n",
    "you want to filter out the relevant set of results.\n",
    "\n",
    "Lets see how to perform rag on tools like discuss before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c189f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(initial_tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6acbb507",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = obj_retriever.retrieve(\n",
    "    \"Tell me about the eval dataset used in MetaGPT and SWE-Bench\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda1f5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7078bb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.agent import FunctionCallingAgentWorker\n",
    "from llama_index.core.agent import AgentRunner\n",
    "\n",
    "agent_worker = FunctionCallingAgentWorker.from_tools(\n",
    "    tool_retriever = obj_retriever,\n",
    "    llm=llm,\n",
    "    system_prompt=\"\"\"\\\n",
    "        Your are an agent designed to answer queries over a set of given papers.\n",
    "        Please always use the tools provided to answer a question. Do not rely on prior knowledge.\\\n",
    "        \"\"\",\n",
    "    verbose = True\n",
    ")\n",
    "\n",
    "agent = AgentRunner(agent_worker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9500e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = agent.query(\n",
    "    \"Tell me about the evaluation dataset used \"\n",
    "    \"in MetaGPT and compare it against SWE-Bench\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21579b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = agent.query(\n",
    "    \"Compare and contrast the LoRa papers(LongLoRa, LoftQ). \"\n",
    "    \"Analyze the approach in each paper first.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c27f856",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26684d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06baa24b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7355d3fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "enve",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
